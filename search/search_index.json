{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#tamedia-kubernetes-as-a-service-kaas-terraform-module","title":"Tamedia Kubernetes as a Service (KaaS) Terraform Module","text":"<p>Opinionated Terraform module to deploy Kubernetes in AWS. Includes:</p> <p>Managed Addons:</p> <ul> <li>EBS CSI</li> <li>VPC CNI</li> <li>CoreDNS</li> <li>KubeProxy</li> </ul> <p>Components (installed by default):</p> <ul> <li>Karpenter</li> <li>ArgoCD</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<p>The module needs some resources to be deployed in order to operate correctly:</p> <p>IAM service-linked roles</p> <ul> <li>AWSServiceRoleForEC2Spot</li> <li>AWSServiceRoleForEC2SpotFleet</li> </ul>"},{"location":"#usage","title":"Usage","text":"<pre><code>module \"k8s_platform\" {\n  source = \"tx-pts-dai/kubernetes-platform/aws\"\n  # Pin this module to a specific version to avoid breaking changes\n  # version = \"0.0.0\"\n\n  name = \"example-platform\"\n\n  vpc = {\n    vpc_id          = \"vpc-12345678\"\n    vpc_cidr        = \"10.0.0.0/16\"\n    private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\n    intra_subnets   = [\"10.0.3.0/24\"]\n  }\n\n  tags = {\n    Environment = \"sandbox\"\n    GithubRepo  = \"terraform-aws-kubernetes-platform\"\n  }\n}\n</code></pre> <p>See the Examples below for more use cases</p>"},{"location":"#release-new-kubernetes-version","title":"Release new kubernetes version","text":"<p>important Each new kubernetes version needs it's own release. This is due to the fact that we should not skip kubernetes versions during a cluster upgrade.</p> <p>To release a new Kubernetes version, follow these steps:</p> <ol> <li>Update the version file:</li> <li>Open the <code>K8S_VERSION</code> file located in the root of the repository.</li> <li> <p>Update the version number to the next Kubernetes version.</p> </li> <li> <p>Commit the Changes:</p> </li> <li> <p>Commit the changes to the <code>K8S_VERSION</code> file with a meaningful commit message following the release proces. For example:      <pre><code>git add K8S_VERSION\ngit commit -m \"feat! update Kubernetes version to 1.30\"\n</code></pre></p> </li> <li> <p>Push the Changes:</p> </li> <li> <p>Push the changes to the main branch, the release workflow will automatically run. This workflow will:</p> <ul> <li>Read the updated Kubernetes version from the <code>K8S_VERSION</code> file.</li> <li>Determine the new module version based on the commit message.</li> <li>Create a new release with the updated module version and the kubernetes version as metadata. The format would be X.Y.Z+A.B where X.Y.Z is the module version and A.B is the kubenetes control plane version.</li> </ul> </li> <li> <p>Verify the Release:</p> </li> <li>Check the GitHub Actions page to ensure the release workflow completed successfully.</li> <li>Verify that the new module version is available in the Terraform Registry.</li> </ol>"},{"location":"#explanation-and-description-of-interesting-use-cases","title":"Explanation and description of interesting use-cases","text":"<p>Why this module?</p> <ul> <li>To provide an AWS account with a K8s cluster with batteries included so that you can start deploying your workloads on a well-built foundation</li> <li>To encourage standardization and common practices</li> <li>To ease maintenance</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Complete - Includes creation of VPC, k8s cluster, addons and all the optional features.</li> <li>Datadog - EKS deployment with Datadog Operator integration</li> <li>Lacework - EKS deployment with Lacework integration</li> <li>Network - VPC deployment with custom subnets for kubernetes</li> </ul>"},{"location":"#cleanup-example-deployments","title":"Cleanup example deployments","text":"<p>Destroy Workflow - This manual workflow destroys deployed example deployments by selection the branch and the example to destroy.</p>"},{"location":"#contributing","title":"Contributing","text":""},{"location":"#pre-commit","title":"Pre-Commit","text":"<p>Installation: install pre-commit and execute <code>pre-commit install</code>. This will generate pre-commit hooks according to the config in <code>.pre-commit-config.yaml</code></p> <p>Before submitting a PR be sure to have used the pre-commit hooks or run: <code>pre-commit run -a</code></p> <p>The <code>pre-commit</code> command will run:</p> <ul> <li>Terraform fmt</li> <li>Terraform validate</li> <li>Terraform docs</li> <li>Terraform validate with tflint</li> <li>check for merge conflicts</li> <li>fix end of files</li> </ul> <p>as described in the <code>.pre-commit-config.yaml</code> file</p>"},{"location":"#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.10 aws &gt;= 6.9 helm &gt;= 3.0.2 kubectl &gt;= 2.0.2 kubernetes &gt;= 2.27 time &gt;= 0.11"},{"location":"#providers","title":"Providers","text":"Name Version aws &gt;= 6.9 helm &gt;= 3.0.2 kubernetes &gt;= 2.27 time &gt;= 0.11"},{"location":"#modules","title":"Modules","text":"Name Source Version acm terraform-aws-modules/acm/aws 6.1.0 argocd ./modules/argocd n/a aws_ebs_csi_pod_identity terraform-aws-modules/eks-pod-identity/aws 2.0.0 aws_gateway_controller_pod_identity terraform-aws-modules/eks-pod-identity/aws 2.0.0 aws_lb_controller_pod_identity terraform-aws-modules/eks-pod-identity/aws 2.0.0 aws_vpc_cni_pod_identity terraform-aws-modules/eks-pod-identity/aws 2.0.0 ebs_csi_driver_irsa terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts 6.2.1 eks terraform-aws-modules/eks/aws 21.1.5 eks_addons ./modules/eks-addons n/a external_dns_pod_identity terraform-aws-modules/eks-pod-identity/aws 2.0.0 external_secrets_pod_identity terraform-aws-modules/eks-pod-identity/aws 2.0.0 karpenter terraform-aws-modules/eks/aws//modules/karpenter 21.1.5 karpenter_irsa terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts 6.2.1 karpenter_security_group ./modules/security-group n/a ssm ./modules/ssm n/a vpc_cni_irsa terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts 6.2.1"},{"location":"#resources","title":"Resources","text":"Name Type aws_cloudwatch_log_group.fargate_fluentbit resource aws_iam_policy.fargate_fluentbit resource aws_iam_policy.karpenter_controller resource aws_route_table_association.karpenter resource aws_security_group_rule.eks_control_plane_ingress resource aws_subnet.karpenter resource helm_release.karpenter_crd resource helm_release.karpenter_release resource helm_release.karpenter_resources resource kubernetes_config_map_v1.aws_logging resource kubernetes_namespace_v1.aws_observability resource time_sleep.wait_after_karpenter resource time_sleep.wait_on_destroy resource time_static.timestamp_id resource aws_availability_zones.available data source aws_caller_identity.current data source aws_iam_policy_document.fargate_fluentbit data source aws_iam_policy_document.karpenter_controller data source aws_iam_roles.sso data source aws_region.current data source aws_route53_zone.base_domain_zone data source aws_route_tables.private_route_tables data source"},{"location":"#inputs","title":"Inputs","text":"Name Description Type Default Required acm_certificate ACM certificate configuration for the domain(s). Controls domain name, alternative domain names, wildcard configuration, and validation behavior.Options include:  - domain_name: Primary domain name for the certificate. If not provided, uses base_domain from other configuration.  - subject_alternative_names: List of additional domain names to include in the certificate.  - wildcard_certificates: When true, adds a wildcard prefix (*.) to all domains in the certificate.  - prepend_stack_id: When true, prepends the stack identifier to each domain name. Only works after random_string is created.  - wait_for_validation: When true, Terraform will wait for certificate validation to complete before proceeding. <pre>object({    domain_name               = optional(string)    subject_alternative_names = optional(list(string), [])    wildcard_certificates     = optional(bool, false)    prepend_stack_id          = optional(bool, false)    wait_for_validation       = optional(bool, false)  })</pre> <code>{}</code> no argocd Argo CD configurations <pre>object({    # Hub specific    enable_hub        = optional(bool, false)    namespace         = optional(string, \"argocd\")    hub_iam_role_name = optional(string, \"argocd-controller\")    # Spoke specific    enable_spoke = optional(bool, false)    hub_iam_role_arn  = optional(string, null)    hub_iam_role_arns = optional(list(string), null)    # Common    tags = optional(map(string), {})  })</pre> <code>{}</code> no base_domain Base domain for the platform, used for ingress and ACM certificates <code>string</code> <code>null</code> no cluster_admins Map of IAM roles to add as cluster admins  role_arn: ARN of the IAM role to add as cluster admin  role_name: Name of the IAM role to add as cluster admin  kubernetes_groups: List of Kubernetes groups to add the role to (default: [\"system:masters\"])role_arn and role_name are mutually exclusive, exactly one must be set. <pre>map(object({    role_arn          = optional(string)    role_name         = optional(string)    kubernetes_groups = optional(list(string))  }))</pre> <code>{}</code> no create_addon_pod_identity_roles Create addon pod identities roles. If set to true, all roles will be created <code>bool</code> <code>true</code> no eks Map of EKS configurations <code>any</code> <code>{}</code> no enable_acm_certificate Enable ACM certificate <code>bool</code> <code>false</code> no enable_argocd Enable Argo CD <code>bool</code> <code>false</code> no enable_fargate_fluentbit Enable Fargate Fluentbit <code>bool</code> <code>true</code> no enable_sso_admin_auto_discovery Enable automatic discovery of SSO admin roles. When disabled, only explicitly defined cluster_admins are used. <code>bool</code> <code>true</code> no enable_timestamp_id Disable the timestamp-based ID generation. When true, uses a static ID instead of timestamp. <code>bool</code> <code>true</code> no extra_cluster_addons Map of cluster addon configurations to enable for the cluster. Addon name can be the map keys or set with <code>name</code>. Addons are created after karpenter resources <code>any</code> <code>{}</code> no extra_cluster_addons_timeouts Create, update, and delete timeout configurations for the cluster addons <code>map(string)</code> <code>{}</code> no karpenter Karpenter configurations <pre>object({    subnet_cidrs = optional(list(string), [])  })</pre> <code>{}</code> no karpenter_helm_set List of Karpenter Helm set values <pre>list(object({    name  = string    value = string    type  = optional(string)  }))</pre> <code>[]</code> no karpenter_helm_values List of Karpenter Helm values <code>list(string)</code> <code>[]</code> no karpenter_resources_helm_set List of Karpenter Resources Helm set values <pre>list(object({    name  = string    value = string    type  = optional(string)  }))</pre> <code>[]</code> no karpenter_resources_helm_values List of Karpenter Resources Helm values <code>list(string)</code> <code>[]</code> no name The name of the platform, a timestamp will be appended to this name to make the stack_name. If not provided, the name of the directory will be used. <code>string</code> <code>\"\"</code> no region AWS region to use <code>string</code> <code>null</code> no tags Default tags to apply to all resources <code>map(string)</code> <code>{}</code> no vpc VPC configurations <pre>object({    vpc_id          = string    vpc_cidr        = string    private_subnets = list(string)    intra_subnets   = list(string)  })</pre> n/a yes"},{"location":"#outputs","title":"Outputs","text":"Name Description argocd Map of attributes for the ArgoCD module eks Map of attributes for the EKS cluster karpenter Map of attributes for the Karpenter module"},{"location":"#authors","title":"Authors","text":"<p>Module is maintained by Alfredo Gottardo, David Beauvererd, Davide Cammarata, Francisco Ferreira,  Roland Bapst and Samuel Wibrow</p>"},{"location":"#license","title":"License","text":"<p>Apache 2 Licensed. See LICENSE for full details.</p>"},{"location":"argocd-addons-migration/","title":"ArgoCD Addons Migration Guide","text":""},{"location":"argocd-addons-migration/#overview","title":"Overview","text":"<p>This guide documents the migration process for deploying Kubernetes cluster addons using ArgoCD ApplicationSets with a label-based enablement strategy. This approach provides a scalable, GitOps-driven method for managing cluster tooling across multiple environments.</p>"},{"location":"argocd-addons-migration/#architecture","title":"Architecture","text":""},{"location":"argocd-addons-migration/#core-components","title":"Core Components","text":"<ol> <li>Main Application: <code>cluster-tools-apps</code> in <code>/configs/cluster-tools/cluster-tools.yaml</code></li> <li>Manages all addon ApplicationSets</li> <li>Recursively processes all <code>appset.yaml</code> files in the apps directory</li> <li> <p>Deployed to the <code>argocd</code> namespace</p> </li> <li> <p>ApplicationSets: Individual addon configurations in <code>/configs/cluster-tools/apps/</code></p> </li> <li>Each addon has its own directory with an <code>appset.yaml</code></li> <li>Uses cluster generator with label selectors</li> <li>Supports environment and team-specific configurations</li> </ol>"},{"location":"argocd-addons-migration/#label-based-addon-installation","title":"Label-Based Addon Installation","text":"<p>Addons are automatically deployed to clusters based on labels. Each addon's ApplicationSet uses a cluster generator that matches specific labels:</p> <pre><code>generators:\n- clusters:\n    selector:\n      matchLabels:\n        enable-&lt;addon-name&gt;: \"true\"\n</code></pre>"},{"location":"argocd-addons-migration/#migration-process","title":"Migration Process","text":""},{"location":"argocd-addons-migration/#step-1-prepare-cluster-labels","title":"Step 1: Prepare Cluster Labels","text":"<p>Label your clusters in ArgoCD to enable specific addons:</p>"},{"location":"argocd-addons-migration/#step-2-create-addon-applicationset","title":"Step 2: Create Addon ApplicationSet","text":"<p>For each addon, create a directory structure:</p> <pre><code>configs/cluster-tools/apps/&lt;addon-name&gt;/\n\u251c\u2500\u2500 appset.yaml           # ApplicationSet definition\n\u251c\u2500\u2500 values.yaml           # Default values (optional)\n\u251c\u2500\u2500 values-&lt;env&gt;.yaml     # Environment-specific values (optional)\n\u2514\u2500\u2500 values-&lt;team&gt;.yaml    # Team-specific values (optional)\n\u2514\u2500\u2500 values-&lt;team&gt;-&lt;env&gt;.yaml # Team and environment specific values (optional)\n</code></pre> <p>Example ApplicationSet template (<code>appset.yaml</code>):</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: &lt;addon-name&gt;\n  namespace: argocd\n  labels:\n    team: platform\nspec:\n  goTemplate: true\n  goTemplateOptions: [\"missingkey=error\"]\n  generators:\n  - clusters:\n      selector:\n        matchLabels:\n          enable-&lt;addon-name&gt;: \"true\"\n  template:\n    metadata:\n      name: \"{{.nameNormalized}}-&lt;addon-name&gt;\"\n      labels:\n        team: \"{{.metadata.labels.team}}\"\n        app: &lt;addon-name&gt;\n        environment: \"{{.metadata.labels.environment}}\"\n      finalizers:\n        - resources-finalizer.argocd.argoproj.io\n    spec:\n      project: cluster-tools\n      sources:\n      - repoURL: &lt;helm-chart-repo&gt;\n        chart: &lt;chart-name&gt;\n        targetRevision: &lt;version&gt;\n        helm:\n          releaseName: &lt;addon-name&gt;\n          namespace: &lt;target-namespace&gt;\n          valueFiles:\n          - $values/values.yaml\n          - $values/values-{{.metadata.labels.environment}}.yaml\n          - $values/values-{{.metadata.labels.team}}.yaml\n          - $values/values-{{.metadata.labels.team}}-{{.metadata.labels.environment}}.yaml\n          ignoreMissingValueFiles: true\n      - repoURL: https://github.com/dnd-it/fission-argocd.git\n        targetRevision: HEAD\n        path: configs/cluster-tools/apps/&lt;addon-name&gt;\n        directory:\n          exclude: appset.yaml\n        ref: values\n      destination:\n        server: \"{{.server}}\"\n        namespace: &lt;target-namespace&gt;\n      syncPolicy:\n        automated:\n          prune: true\n          selfHeal: true\n          allowEmpty: true\n        syncOptions:\n          - CreateNamespace=true\n          - ApplyOutOfSyncOnly=true\n          - PruneLast=true\n          - SkipDryRunOnMissingResource=true\n          - ServerSideApply=true\n</code></pre>"},{"location":"argocd-addons-migration/#step-3-configure-values-hierarchy","title":"Step 3: Configure Values Hierarchy","text":"<p>The system supports a hierarchical values configuration:</p> <ol> <li><code>values.yaml</code> - Base configuration for all deployments</li> <li><code>values-&lt;environment&gt;.yaml</code> - Environment-specific overrides (dev, staging, production)</li> <li><code>values-&lt;team&gt;.yaml</code> - Team-specific configurations</li> <li><code>values-&lt;team&gt;-&lt;environment&gt;.yaml</code> - Team and environment specific combinations</li> </ol> <p>Files are merged in order, with later files overriding earlier ones. Missing files are ignored (<code>ignoreMissingValueFiles: true</code>).</p>"},{"location":"faq/","title":"FAQ","text":"<p>tba</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This getting started guide will help you deploy your first EKS cluster.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools locally:</p> <ul> <li>awscli</li> <li>kubectl</li> <li>terraform</li> </ul>"},{"location":"getting-started/#deploy","title":"Deploy","text":"<ol> <li> <p>For consuming EKS Blueprints, please see the Consumption section. For exploring and trying out the patterns provided, please clone the project locally to quickly get up and running with a pattern. After cloning the project locally, <code>cd</code> into the pattern directory of your choice.</p> </li> <li> <p>To provision the pattern, the typical steps of execution are as follows:</p> <pre><code>terraform init\nterraform apply -auto-approve\n</code></pre> <p>For patterns that deviate from this general flow, see the pattern's respective <code>README.md</code> for more details.</p> <p>Terraform targeted apply</p> <p>Please see the Terraform Caveats section for details on the use of targeted Terraform apply's</p> </li> <li> <p>Once all of the resources have successfully been provisioned, the following command can be used to update the <code>kubeconfig</code> on your local machine and allow you to interact with your EKS Cluster using <code>kubectl</code>.</p> <pre><code>aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt; --alias &lt;CLUSTER_NAME&gt;\n</code></pre> <p>Pattern Terraform outputs</p> <p>Most examples will output the <code>aws eks update-kubeconfig ...</code> command as part of the Terraform apply output to simplify this process for users</p> <p>Private clusters</p> <p>Clusters that do not enable the clusters public endpoint will require users to access the cluster from within the VPC. For these patterns, a sample EC2 or other means are provided to demonstrate how to access those clusters privately</p> <p>and without exposing the public endpoint. Please see the respective pattern's <code>README.md</code> for more details.</p> </li> <li> <p>Once you have updated your <code>kubeconfig</code>, you can verify that you are able to interact with your cluster by running the following command:</p> <pre><code>kubectl get nodes\n</code></pre> <p>This should return a list of the node(s) running in the cluster created. If any errors are encountered, please re-trace the steps above and consult the pattern's <code>README.md</code> for more details on any additional/specific steps that may be required.</p> </li> </ol>"},{"location":"getting-started/#destroy","title":"Destroy","text":"<p>To teardown and remove the resources created in the pattern, the typical steps of execution are as follows:</p> <pre><code>terraform destroy -auto-approve\n</code></pre> <p>Resources created outside of Terraform</p> <p>Depending on the pattern, some resources may have been created that Terraform is not aware of that will cause issues when attempting to clean up the pattern. For example, Karpenter is responsible for creating additional EC2 instances to satisfy the pod scheduling requirements. These instances will not be cleaned up by Terraform and will need to be de-provisioned BEFORE attempting to <code>terraform destroy</code>. This is why it is important that the addons, or any resources provisioned onto the cluster are cleaned up first. Please see the respective pattern's <code>README.md</code> for more details.</p>"},{"location":"examples/argocd/","title":"Argocd","text":"<pre><code>terraform {\n  required_version = \"~&gt; 1.10\"\n\n  backend \"s3\" {\n    bucket               = \"tf-state-911453050078\"\n    key                  = \"modules/argocd/examples/simple.tfstate\"\n    workspace_key_prefix = \"terraform-aws-kubernetes-platform\"\n    region               = \"eu-central-1\"\n    use_lockfile         = true\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 6.9\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"~&gt; 3.0\"\n    }\n    kubectl = {\n      source  = \"alekc/kubectl\"\n      version = \"~&gt; 2.0\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~&gt; 2.27\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = local.region\n}\n\nprovider \"kubernetes\" {\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nprovider \"helm\" {\n  kubernetes = {\n    host                   = module.k8s_platform.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n    exec = {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      command     = \"aws\"\n      args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n    }\n  }\n}\n\nlocals {\n  region = \"eu-central-1\"\n}\n\nmodule \"k8s_platform\" {\n  source = \"./../../..\"\n\n  name = \"ex-argocd\"\n\n  cluster_admins = {\n    cicd = {\n      role_name = \"cicd-iac\"\n    }\n  }\n\n  tags = {\n    Environment = \"sandbox\"\n    GithubRepo  = \"terraform-aws-kubernetes-platform\"\n    GithubOrg   = \"tx-pts-dai\"\n  }\n\n  vpc = {\n    vpc_id   = \"vpc-12345678\"\n    vpc_cidr = \"10.0.0.0/16\"\n    private_subnets = [\n      \"subnet-12345678\",\n      \"subnet-23456789\",\n    ]\n    intra_subnets = [\n      \"subnet-34567890\",\n      \"subnet-45678901\",\n    ]\n  }\n}\n\nmodule \"hub\" {\n  source = \"./..\"\n\n  enable_hub = true\n\n  cluster_name = module.k8s_platform.eks.cluster_name\n}\n\nmodule \"spoke\" {\n  source = \"./..\"\n\n  enable_spoke = true\n\n  cluster_name = module.k8s_platform.eks.cluster_name\n\n  hub_iam_role_arn = module.hub.hub_iam_role_arn\n\n  hub_iam_role_arns = [\"arn:aws:iam::123456789012:role/another-role\"]\n}\n</code></pre>"},{"location":"examples/complete/","title":"Complete","text":"<pre><code>terraform {\n  required_version = \"~&gt; 1.10\"\n\n  backend \"s3\" {\n    bucket               = \"tf-state-911453050078\"\n    key                  = \"examples/complete.tfstate\"\n    workspace_key_prefix = \"terraform-aws-kubernetes-platform\"\n    use_lockfile         = true\n    region               = \"eu-central-1\"\n    encrypt              = true\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 6.9\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"~&gt; 3.0\"\n    }\n    kubectl = {\n      source  = \"alekc/kubectl\"\n      version = \"~&gt; 2.0\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~&gt; 2.27\"\n    }\n    cloudflare = {\n      source  = \"cloudflare/cloudflare\"\n      version = \"~&gt; 4.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = local.region\n}\n\nprovider \"kubernetes\" {\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nprovider \"helm\" {\n  kubernetes = {\n    host                   = module.k8s_platform.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n    exec = {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      command     = \"aws\"\n      args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n    }\n  }\n}\n\nprovider \"kubectl\" {\n  apply_retry_count      = 5\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  load_config_file       = false\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nlocals {\n  region = \"eu-central-1\"\n}\n\ndata \"aws_vpc\" \"default\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"central\"]\n  }\n}\n\ndata \"aws_subnets\" \"private_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*private*\"]\n  }\n}\n\ndata \"aws_subnets\" \"intra_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*intra*\"]\n  }\n}\n\nmodule \"k8s_platform\" {\n  source = \"../../\"\n\n  name = \"ex-complete\"\n\n  cluster_admins = {\n    cicd = {\n      role_name = \"cicd-iac\"\n    }\n  }\n\n  tags = {\n    Environment = \"sandbox\"\n    GithubRepo  = \"terraform-aws-kubernetes-platform\"\n    GithubOrg   = \"tx-pts-dai\"\n  }\n\n  vpc = {\n    vpc_id          = data.aws_vpc.default.id\n    vpc_cidr        = data.aws_vpc.default.cidr_block\n    private_subnets = data.aws_subnets.private_subnets.ids\n    intra_subnets   = data.aws_subnets.intra_subnets.ids\n  }\n\n  karpenter_resources_helm_set = [\n    {\n      name  = \"global.eksDiscovery.tags.subnets.karpenter\\\\.sh/discovery\"\n      value = \"shared\"\n    }\n  ]\n}\n\ndata \"aws_secretsmanager_secret_version\" \"cloudflare\" {\n  secret_id = \"dai/cloudflare/tamedia/apiToken\"\n}\n\nprovider \"cloudflare\" {\n  api_token = jsondecode(data.aws_secretsmanager_secret_version.cloudflare.secret_string)[\"apiToken\"]\n}\n\nlocals {\n  zones = {\n    \"kaas-example.tamedia.tech\" = {\n      comment = \"DAI KaaS example complete\"\n    }\n  }\n}\n\n# Manage DNS sub-domains in cloudflare and attach them to they parent in route53\nmodule \"cloudflare\" {\n  source = \"../../modules/cloudflare\"\n\n  for_each = local.zones\n\n  zone_name    = module.route53_zones[each.key].route53_zone_name[each.key]\n  comment      = \"Managed by KAAS examples\"\n  name_servers = [for i in range(4) : module.route53_zones[each.key].route53_zone_name_servers[each.key][i]]\n  account_id   = jsondecode(data.aws_secretsmanager_secret_version.cloudflare.secret_string)[\"accountId\"]\n}\n\nmodule \"route53_zones\" {\n  source  = \"terraform-aws-modules/route53/aws//modules/zones\"\n  version = \"5.0.0\"\n\n  for_each = local.zones\n\n  zones = {\n    (each.key) = {\n      comment = each.value.comment\n    }\n  }\n}\noutput \"eks\" {\n  description = \"eks module outputs\"\n  value       = module.k8s_platform.eks\n}\n\noutput \"zconfigure_kubectl\" {\n  description = \"Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig\"\n  value       = \"aws eks --region ${local.region} update-kubeconfig --name ${module.k8s_platform.eks.cluster_name} --alias ${module.k8s_platform.eks.cluster_name}\"\n}\n\noutput \"karpenter\" {\n  description = \"karpenter module outputs\"\n  value       = module.k8s_platform.karpenter\n}\n</code></pre>"},{"location":"examples/datadog/","title":"Datadog","text":"<pre><code>terraform {\n  required_version = \"~&gt; 1.10\"\n\n  backend \"s3\" {\n    bucket               = \"tf-state-911453050078\"\n    key                  = \"examples/datadog.tfstate\"\n    workspace_key_prefix = \"terraform-aws-kubernetes-platform\"\n    use_lockfile         = true\n    region               = \"eu-central-1\"\n    encrypt              = true\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 6.9\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~&gt; 2.27\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"~&gt; 3.0\"\n    }\n    kubectl = {\n      source  = \"alekc/kubectl\"\n      version = \"~&gt; 2.0\"\n    }\n    time = {\n      source  = \"hashicorp/time\"\n      version = \"~&gt; 0.11\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = local.region\n}\n\n\nprovider \"kubernetes\" {\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nprovider \"helm\" {\n  kubernetes = {\n    host                   = module.k8s_platform.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n    exec = {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      command     = \"aws\"\n      args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n    }\n  }\n}\n\nprovider \"kubectl\" {\n  apply_retry_count      = 5\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  load_config_file       = false\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nlocals {\n  region = \"eu-central-1\"\n}\n\ndata \"aws_vpc\" \"default\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"central\"]\n  }\n}\n\ndata \"aws_subnets\" \"private_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*private*\"]\n  }\n}\n\ndata \"aws_subnets\" \"intra_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*intra*\"]\n  }\n}\n\nmodule \"k8s_platform\" {\n  source = \"../../\"\n\n  name = \"ex-datadog\"\n\n  cluster_admins = {\n    cicd = {\n      role_name = \"cicd-iac\"\n    }\n  }\n\n  vpc = {\n    vpc_id          = data.aws_vpc.default.id\n    vpc_cidr        = data.aws_vpc.default.cidr_block\n    private_subnets = data.aws_subnets.private_subnets.ids\n    intra_subnets   = data.aws_subnets.intra_subnets.ids\n  }\n\n  karpenter_helm_values = [\n    &lt;&lt;-YAML\n      podAnnotations:\n        \"ad.datadoghq.com/controller.checks\": |\n          karpenter:\n            init_config: {}\n            instances:\n              - openmetrics_endpoint: http://%%host%%:8080/metrics\n      controller:\n        resources:\n          requests:\n            cpu: 0.5\n            memory: \"768Mi\"\n      YAML\n  ]\n\n  karpenter_resources_helm_set = [\n    {\n      name  = \"global.eksDiscovery.tags.subnets.karpenter\\\\.sh/discovery\"\n      value = \"shared\"\n    }\n  ]\n\n  tags = {\n    Environment = \"sandbox\"\n    GithubRepo  = \"terraform-aws-kubernetes-platform\"\n    GithubOrg   = \"tx-pts-dai\"\n  }\n}\n\nmodule \"datadog\" {\n  source = \"../../modules/datadog\"\n\n  cluster_name   = module.k8s_platform.eks.cluster_name\n  datadog_secret = \"dai/datadog/tamedia/keys\"\n  environment    = \"sandbox\"\n  product_name   = \"dai\"\n\n  datadog_operator_helm_values = [\n    &lt;&lt;-YAML\n    remoteConfiguration:\n      enabled: true\n    YAML\n  ]\n\n\n  datadog_operator_helm_set = [\n    {\n      name  = \"replicas\"\n      value = 2\n    }\n  ]\n\n  # Example: how to override specs in the Datadog Custom Resource\n  # How to update the number of clusterAgent replicas and enable datadog agent as external metrics server\n  datadog_agent_helm_values = [\n    &lt;&lt;-YAML\n    spec:\n      features:\n        externalMetricsServer:\n          enabled: true\n          useDatadogMetrics: true\n      override:\n        clusterAgent:\n          replicas: 1\n    YAML\n  ]\n\n  datadog_agent_helm_set = [\n    {\n      name  = \"spec.features.admissionController.agentSidecarInjection.image.tag\",\n      value = \"7.57.2\"\n    }\n  ]\n\n  depends_on = [module.k8s_platform]\n}\noutput \"eks\" {\n  description = \"eks module outputs\"\n  value       = module.k8s_platform.eks\n}\n\noutput \"zconfigure_kubectl\" {\n  description = \"Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig\"\n  value       = \"aws eks --region ${local.region} update-kubeconfig --name ${module.k8s_platform.eks.cluster_name} --alias ${module.k8s_platform.eks.cluster_name}\"\n}\n</code></pre>"},{"location":"examples/lacework/","title":"Lacework","text":"<pre><code>terraform {\n  required_version = \"~&gt; 1.10\"\n\n  backend \"s3\" {\n    bucket               = \"tf-state-911453050078\"\n    key                  = \"examples/lacework.tfstate\"\n    workspace_key_prefix = \"terraform-aws-kubernetes-platform\"\n    use_lockfile         = true\n    region               = \"eu-central-1\"\n    encrypt              = true\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 6.9\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~&gt; 2.27\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"~&gt; 3.0\"\n    }\n    kubectl = {\n      source  = \"alekc/kubectl\"\n      version = \"~&gt; 2.0\"\n    }\n    lacework = {\n      source  = \"lacework/lacework\"\n      version = \"~&gt; 2.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = local.region\n}\n\nprovider \"kubernetes\" {\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nprovider \"helm\" {\n  kubernetes = {\n    host                   = module.k8s_platform.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n    exec = {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      command     = \"aws\"\n      args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n    }\n  }\n}\n\nprovider \"kubectl\" {\n  apply_retry_count      = 5\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  load_config_file       = false\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\ndata \"aws_secretsmanager_secret\" \"lacework\" {\n  name = \"dai/lacework/tamedia/apiKey\"\n}\n\ndata \"aws_secretsmanager_secret_version\" \"lacework\" {\n  secret_id = data.aws_secretsmanager_secret.lacework.id\n}\n\nprovider \"lacework\" {\n  account    = jsondecode(data.aws_secretsmanager_secret_version.lacework.secret_string)[\"account\"]\n  subaccount = jsondecode(data.aws_secretsmanager_secret_version.lacework.secret_string)[\"subAccount\"]\n  api_key    = jsondecode(data.aws_secretsmanager_secret_version.lacework.secret_string)[\"keyId\"]\n  api_secret = jsondecode(data.aws_secretsmanager_secret_version.lacework.secret_string)[\"secret\"]\n}\n\nlocals {\n  region = \"eu-central-1\"\n}\n\ndata \"aws_vpc\" \"default\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"central\"]\n  }\n}\n\ndata \"aws_subnets\" \"private_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*private*\"]\n  }\n}\n\ndata \"aws_subnets\" \"intra_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*intra*\"]\n  }\n}\n\nmodule \"k8s_platform\" {\n  source = \"../../\"\n\n  name = \"ex-lacework\"\n\n  cluster_admins = {\n    cicd = {\n      role_name = \"cicd-iac\"\n    }\n  }\n\n  vpc = {\n    vpc_id          = data.aws_vpc.default.id\n    vpc_cidr        = data.aws_vpc.default.cidr_block\n    private_subnets = data.aws_subnets.private_subnets.ids\n    intra_subnets   = data.aws_subnets.intra_subnets.ids\n  }\n\n  karpenter_resources_helm_set = [\n    {\n      name  = \"global.eksDiscovery.tags.subnets.karpenter\\\\.sh/discovery\"\n      value = \"shared\"\n    }\n  ]\n\n  tags = {\n    Environment = \"sandbox\"\n    GithubRepo  = \"terraform-aws-kubernetes-platform\"\n    GithubOrg   = \"tx-pts-dai\"\n  }\n\n}\n\nmodule \"lacework\" {\n  source = \"../../modules/lacework\"\n\n  cluster_name = module.k8s_platform.eks.cluster_name\n\n  agent_tags = {\n    KubernetesCluster = module.k8s_platform.eks.cluster_name\n  }\n}\noutput \"eks\" {\n  description = \"eks module outputs\"\n  value       = module.k8s_platform.eks\n}\n\noutput \"zconfigure_kubectl\" {\n  description = \"Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig\"\n  value       = \"aws eks --region ${local.region} update-kubeconfig --name ${module.k8s_platform.eks.cluster_name} --alias ${module.k8s_platform.eks.cluster_name}\"\n}\n</code></pre>"},{"location":"examples/snippets/karpenter/","title":"Karpenter","text":"<p>By default the Kubernetes as a Service (KaaS) module will deploy a Karpenter controller and the custom resources for creating nodes. These resources are NodePool and Ec2NodeClass. You can think of these as NodeClass defines the ec2 launch template and NodePool defines the node group. The Karpenter controller will then create the nodes based on the node class and node pool.</p>"},{"location":"examples/snippets/karpenter/#configuration","title":"Configuration","text":"<p>To customise the Karpenter configuration, you need to define the necessary variables in the <code>kaas</code> module. The following example shows how to do this:</p> <pre><code>module \"k8s_platform\" {\n  source = \"../../\"\n\n  name = \"ex-complete\"\n\n  ...redacted for brevity...\n\n  # Karpenter general configuration variable\n  karpenter = {\n    values = [\n      # Add your custom values here\n      # These will be treated like how helm treat values inputs. eg. helm install -f values.yaml -f values2.yaml\n      replicas: 2\n    ]\n    # Karpenter Helm chart sets\n    set = [\n      {\n        name  = \"replicas\"\n        value = 1\n      }\n    ]\n\n\n## Karpenter Custom Resources\n\nThe Karpenter custom resources are defined in the `karpenter_resources` variable. This variable is a map of values that will be passed to the [Karpenter resources Helm chart](https://github.com/DND-IT/helm-charts/tree/main/charts/karpenter-resources#karpenter-resources). The following example shows how to define the Karpenter custom resources:\n\n```hcl\nmodule \"k8s_platform\" {\n  source = \"../../\"\n\n  name = \"ex-complete\"\n\n  ...redacted for brevity...\n\n  # Karpenter general configuration variable\n  karpenter = {\n    # Karpenter custom resources overrides\n    # Direct inline overrides to the Karpenter custom resources\n    karpenter_resources = {\n      values = [\n        file(\"${path.module}/karpenter-values.yaml\"), # Set the values file\n        &lt;&lt;-EOT # Inline YAML\n        nodePools:\n          default:\n            requirements:\n              - key: karpenter.k8s.aws/instance-category\n                operator: In\n                values: [\"t\"]\n        EOT\n      ]\n      set = [ # Set the values directly\n        {\n          name  = \"nodePools.default.requirements[0].key\"\n          value = \"karpenter.k8s.aws/instance-category\"\n        },\n        {\n          name  = \"nodePools.default.requirements[0].operator\"\n          value = \"In\"\n        },\n        {\n          name  = \"nodePools.default.requirements[0].values\"\n          value = \"[\\\"t\\\"]\"\n        }\n      ]\n    }\n  }\n}\n</code></pre> <pre><code>data \"helm_template\" \"karpenter_custom_resources\" {\n  name       = \"karpenter-resources\"\n  chart      = \"karpenter-resources\"\n  version    = \"0.3.1\"\n  repository = \"https://dnd-it.github.io/helm-charts\"\n  namespace  = local.karpenter.namespace\n\n  values = [\n    &lt;&lt;-EOT\n    global:\n      role: ${module.k8s_platform.karpenter.node_iam_role_name}\n      eksDiscovery:\n        enabled: true\n        clusterName: ${module.k8s_platform.eks.cluster_name}\n\n    nodePools:\n      custom:\n        requirements:\n          - key: karpenter.k8s.aws/instance-category\n            operator: In\n            values: [\"t\"]\n        labels:\n          my-custom-nodepool: \"true\"\n        taints:\n          - key: \"karpenter.sh/capacity-type\"\n            value: \"spot\"\n            effect: \"NoSchedule\"\n        limits:\n          resources:\n            cpu: 100\n            memory: 10Gi\n        providerRef:\n          name: custom\n\n    ec2NodeClasses:\n      custom:\n        amiFamily: al2023\n        amiSelectorTerms:\n          - alias: al2023@v20240807\n        tags:\n          Name: \"custom-node-class\"\n        kubelet:\n          maxPods: 10\n    EOT\n  ]\n}\n</code></pre>"},{"location":"modules/argocd/","title":"Argocd","text":""},{"location":"modules/argocd/#argocd","title":"ArgoCD","text":"<p>This module deploys ArgoCD supporting resources for the hub or spoke. All the necessary IAM roles and policies.</p>"},{"location":"modules/argocd/#examples","title":"Examples","text":""},{"location":"modules/argocd/#hub","title":"Hub","text":"<p>The hub is the controller that manages the spoke clusters. This is where the applications are defined and synced to the spoke clusters.</p> <pre><code>module \"hub\" {\n  source = \"./..\"\n\n  enable_hub = true\n\n  cluster_name = \"example-cluster\"\n}\n</code></pre>"},{"location":"modules/argocd/#spoke","title":"Spoke","text":"<p>The spoke is the controller that manages the applications on the cluster. This is where the applications are deployed and synced from the hub.</p> <pre><code>module \"spoke\" {\n  source = \"./..\"\n\n  enable_spoke = true\n\n  cluster_name = \"example-cluster\"\n\n  hub_iam_role_arn = \"arn:aws:iam::123456789012:role/argocd-example-cluster-hub\"\n}\n</code></pre>"},{"location":"modules/argocd/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.10 aws &gt;= 5.0"},{"location":"modules/argocd/#providers","title":"Providers","text":"Name Version aws &gt;= 5.0"},{"location":"modules/argocd/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/argocd/#resources","title":"Resources","text":"Name Type aws_eks_access_entry.argocd_spoke resource aws_eks_access_policy_association.argocd_spoke resource aws_eks_pod_identity_association.argocd_application_controller resource aws_eks_pod_identity_association.argocd_applicationset_controller resource aws_eks_pod_identity_association.argocd_server resource aws_iam_policy.argocd_controller resource aws_iam_role.argocd_controller resource aws_iam_role.argocd_spoke resource aws_iam_role_policy_attachment.argocd_controller resource aws_eks_cluster.cluster data source aws_iam_policy_document.argocd_controller data source aws_iam_policy_document.argocd_controller_assume_role data source aws_iam_policy_document.argocd_spoke data source"},{"location":"modules/argocd/#inputs","title":"Inputs","text":"Name Description Type Default Required cluster_name Name of the EKS cluster <code>string</code> n/a yes create Create the ArgoCD resources <code>bool</code> <code>true</code> no enable_hub Enable ArgoCD Hub <code>bool</code> <code>false</code> no enable_spoke Enable ArgoCD Spoke <code>bool</code> <code>false</code> no hub_iam_role_arn (Deprecated, use hub_iam_role_arns) IAM Role ARN for ArgoCD Hub. This is required for spoke clusters <code>string</code> <code>null</code> no hub_iam_role_arns A list of ArgoCD Hub IAM Role ARNs, enabling hubs to access spoke clusters. This is required for spoke clusters. <code>list(string)</code> <code>null</code> no hub_iam_role_name IAM Role Name for ArgoCD Hub. This is referenced by the Spoke clusters <code>string</code> <code>\"argocd-controller\"</code> no namespace Namespace to deploy ArgoCD <code>string</code> <code>\"argocd\"</code> no tags A map of tags to add to all resources <code>map(string)</code> <code>{}</code> no"},{"location":"modules/argocd/#outputs","title":"Outputs","text":"Name Description cluster_name Name of the EKS cluster hub_iam_role_arn IAM Role ARN for ArgoCD spoke_iam_role_arn IAM Role ARN for ArgoCD Spoke"},{"location":"modules/cloudflare/","title":"Cloudflare","text":""},{"location":"modules/cloudflare/#cloudflare-zone-delegation","title":"Cloudflare zone delegation","text":"<p>Deploy the Cloudflare delegation</p> <pre><code>module \"cloudflare\" {\n  source   = \"tx-pts-dai/kubernetes-platform/aws//modules/cloudflare\"\n  version  = ...\n  for_each = var.zones\n\n  account_id   = jsondecode(data.aws_secretsmanager_secret_version.cloudflare.secret_string)[\"accountId\"]\n  name_servers = module.route53_zones[each.key].route53_zone_name_servers[each.key]\n  domain_name  = module.route53_zones[each.key].route53_zone_name[each.key]\n}\n</code></pre>"},{"location":"modules/cloudflare/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.10 cloudflare &gt;= 4.0, &lt; 5.0"},{"location":"modules/cloudflare/#providers","title":"Providers","text":"Name Version cloudflare &gt;= 4.0, &lt; 5.0"},{"location":"modules/cloudflare/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/cloudflare/#resources","title":"Resources","text":"Name Type cloudflare_record.ns resource cloudflare_zone.this data source"},{"location":"modules/cloudflare/#inputs","title":"Inputs","text":"Name Description Type Default Required account_id Cloudflare account id <code>string</code> n/a yes comment Record comment <code>string</code> <code>\"Managed by Terraform\"</code> no name_servers List of name servers to delegate to Cloudflare <code>list(string)</code> n/a yes zone_name The domain name to delegate in Cloudflare <code>string</code> n/a yes"},{"location":"modules/cloudflare/#outputs","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/datadog/","title":"Datadog","text":""},{"location":"modules/datadog/#datadog-operator","title":"Datadog Operator","text":"<p>Deploy the Datadog Operator and the Datadog Agent</p> <pre><code>module \"datadog\" {\n  source = \"../../modules/datadog\"\n\n  cluster_name   = \"my-cluster\"\n  datadog_secret = \"secretsmanager/secret/namespace\"\n  environment    = \"example\"\n  product_name   = \"dai\"\n\n  datadog_operator_helm_values = {\n    values = [\n      &lt;&lt;-YAML\n      remoteConfiguration:\n        enabled: true\n      YAML\n    ]\n  }\n\n  datadog_operator_helm_set = [\n    {\n      name  = \"replicas\"\n      value = 2\n    }\n  ]\n\n  datadog_agent_helm_values = [\n    &lt;&lt;-YAML\n    spec:\n      override:\n        clusterAgent:\n          replicas: 1\n    YAML\n  ]\n\n  datadog_agent_helm_set = [\n    {\n      name  = \"spec.features.admissionController.agentSidecarInjection.image.tag\",\n      value = \"7.57.2\"\n    }\n  ]\n}\n</code></pre>"},{"location":"modules/datadog/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.10 aws &gt;= 5.0 helm &gt;= 3.0.2 kubectl &gt;= 2.0.2 kubernetes &gt;= 2.27 time &gt;= 0.11.0"},{"location":"modules/datadog/#providers","title":"Providers","text":"Name Version aws &gt;= 5.0 helm &gt;= 3.0.2 kubectl &gt;= 2.0.2 kubernetes &gt;= 2.27 time &gt;= 0.11.0"},{"location":"modules/datadog/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/datadog/#resources","title":"Resources","text":"Name Type helm_release.datadog_agent resource helm_release.datadog_operator resource kubectl_manifest.fargate_cluster_role resource kubectl_manifest.fargate_role_binding resource kubernetes_annotations.this resource kubernetes_namespace_v1.datadog resource kubernetes_secret.datadog_keys resource time_sleep.this resource aws_secretsmanager_secret.datadog data source aws_secretsmanager_secret_version.datadog data source"},{"location":"modules/datadog/#inputs","title":"Inputs","text":"Name Description Type Default Required cluster_name Name of the cluster <code>string</code> n/a yes datadog_agent_helm_set List of Datadog Agent custom resource set values <pre>list(object({    name  = string    value = string    type  = optional(string)  }))</pre> <code>[]</code> no datadog_agent_helm_values List of Datadog Agent custom resource values <code>list(string)</code> <code>[]</code> no datadog_operator_helm_set List of Datadog Operator Helm set values <pre>list(object({    name  = string    value = string    type  = optional(string)  }))</pre> <code>[]</code> no datadog_operator_helm_values List of Datadog Operator Helm values <code>list(string)</code> <code>[]</code> no datadog_operator_helm_version Version of the datadog operator chart <code>string</code> <code>\"2.12.1\"</code> no datadog_secret Name of the datadog secret in Secrets Manager <code>string</code> n/a yes environment Name of the environment <code>string</code> n/a yes namespace Namespace for Datadog resources <code>string</code> <code>\"monitoring\"</code> no product_name Value of the product tag added to all metrics and logs sent to datadog <code>string</code> n/a yes"},{"location":"modules/datadog/#outputs","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/datadog/#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.0.0 datadog ~&gt; 3.39 helm ~&gt; 2.6 kubernetes &gt;= 2.0.0"},{"location":"modules/datadog/#providers_1","title":"Providers","text":"Name Version datadog ~&gt; 3.39 helm ~&gt; 2.6 kubernetes &gt;= 2.0.0"},{"location":"modules/datadog/#modules_1","title":"Modules","text":"Name Source Version datadog_operator aws-ia/eks-blueprints-addon/aws ~&gt; 1.0"},{"location":"modules/datadog/#resources_1","title":"Resources","text":"Name Type datadog_api_key.datadog_agent resource datadog_application_key.datadog_agent resource helm_release.datadog_agent resource kubernetes_secret.datadog_keys resource"},{"location":"modules/datadog/#inputs_1","title":"Inputs","text":"Name Description Type Default Required cluster_name Name of the cluster <code>string</code> n/a yes datadog Object of Datadog configurations <pre>object({    agent_api_key_name            = optional(string) # by default it uses the cluster name    agent_app_key_name            = optional(string) # by default it uses the cluster name    operator_chart_version        = optional(string)    custom_resource_chart_version = optional(string)  })</pre> <code>{}</code> no datadog_agent_helm_values List of Datadog Agent custom resource values. https://github.com/DataDog/datadog-operator/blob/main/docs/configuration.v2alpha1.md <pre>list(object({    name  = string    value = string  }))</pre> <code>[]</code> no datadog_operator_helm_values List of Datadog Operator values <pre>list(object({    name  = string    value = string  }))</pre> <pre>[  {    \"name\": \"resources.requests.cpu\",    \"value\": \"10m\"  },  {    \"name\": \"resources.requests.memory\",    \"value\": \"50Mi\"  }]</pre> no namespace Namespace for Datadog resources <code>string</code> <code>\"monitoring\"</code> no"},{"location":"modules/datadog/#outputs_1","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/datadog/#requirements_2","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.0.0 datadog ~&gt; 3.39 helm ~&gt; 2.6 kubernetes &gt;= 2.0.0"},{"location":"modules/datadog/#providers_2","title":"Providers","text":"Name Version datadog ~&gt; 3.39 helm ~&gt; 2.6 kubernetes &gt;= 2.0.0"},{"location":"modules/datadog/#modules_2","title":"Modules","text":"Name Source Version datadog_operator aws-ia/eks-blueprints-addon/aws ~&gt; 1.0"},{"location":"modules/datadog/#resources_2","title":"Resources","text":"Name Type datadog_api_key.datadog_agent resource datadog_application_key.datadog_agent resource helm_release.datadog_agent resource kubernetes_secret.datadog_keys resource"},{"location":"modules/datadog/#inputs_2","title":"Inputs","text":"Name Description Type Default Required cluster_name Name of the cluster <code>string</code> n/a yes datadog Object of Datadog configurations <pre>object({    agent_api_key_name            = optional(string) # by default it uses the cluster name    agent_app_key_name            = optional(string) # by default it uses the cluster name    operator_chart_version        = optional(string)    custom_resource_chart_version = optional(string)  })</pre> <code>{}</code> no datadog_agent_helm_values List of Datadog Agent custom resource values. https://github.com/DataDog/datadog-operator/blob/main/docs/configuration.v2alpha1.md <pre>list(object({    name  = string    value = string  }))</pre> <code>[]</code> no datadog_operator_helm_values List of Datadog Operator values <pre>list(object({    name  = string    value = string  }))</pre> <pre>[  {    \"name\": \"resources.requests.cpu\",    \"value\": \"10m\"  },  {    \"name\": \"resources.requests.memory\",    \"value\": \"50Mi\"  }]</pre> no namespace Namespace for Datadog resources <code>string</code> <code>\"monitoring\"</code> no"},{"location":"modules/datadog/#outputs_2","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/eks-addons/","title":"Eks addons","text":""},{"location":"modules/eks-addons/#eks-addons-management","title":"EKS Addons Management","text":"<p>This module manages AWS EKS native addons through Terraform. It handles the lifecycle of EKS-managed addons such as CoreDNS, kube-proxy, VPC CNI, and EBS CSI Driver.</p>"},{"location":"modules/eks-addons/#overview","title":"Overview","text":"<p>The module creates and manages EKS addons using the AWS EKS Addon API. These are AWS-managed Kubernetes components that are essential for cluster operation. This module is typically called after cluster creation and Karpenter setup to avoid dependency issues.</p>"},{"location":"modules/eks-addons/#usage","title":"Usage","text":"<pre><code>module \"eks_addons\" {\n  source = \"./modules/eks-addons\"\n\n  cluster_name       = module.eks.cluster_id\n  kubernetes_version = var.kubernetes_version\n\n  cluster_addons = {\n    coredns = {\n      addon_version            = \"v1.11.1-eksbuild.4\"\n      service_account_role_arn = module.eks.eks_managed_node_groups[\"core\"].iam_role_arn\n    }\n    kube-proxy = {\n      addon_version = \"v1.29.0-eksbuild.1\"\n    }\n    vpc-cni = {\n      addon_version            = \"v1.16.0-eksbuild.1\"\n      service_account_role_arn = module.vpc_cni_irsa.iam_role_arn\n      configuration_values = jsonencode({\n        env = {\n          ENABLE_PREFIX_DELEGATION = \"true\"\n          WARM_PREFIX_TARGET       = \"1\"\n        }\n      })\n    }\n    aws-ebs-csi-driver = {\n      addon_version            = \"v1.28.0-eksbuild.1\"\n      service_account_role_arn = module.ebs_csi_driver_irsa.iam_role_arn\n    }\n  }\n\n  tags = var.tags\n}\n</code></pre>"},{"location":"modules/eks-addons/#inputs","title":"Inputs","text":"Name Description Type Default Required <code>cluster_name</code> The name of the EKS cluster <code>string</code> n/a yes <code>kubernetes_version</code> Kubernetes version to use for the EKS cluster <code>string</code> n/a yes <code>cluster_addons</code> Map of cluster addon configurations <code>map(object)</code> <code>{}</code> no <code>cluster_addons_timeouts</code> Default timeout values for addon resources <code>object</code> <code>{}</code> no <code>tags</code> A map of tags to apply to all resources <code>map(string)</code> <code>{}</code> no"},{"location":"modules/eks-addons/#outputs","title":"Outputs","text":"Name Description <code>addons</code> Map of installed EKS addon attributes"},{"location":"modules/eks-addons/#addon-configuration","title":"Addon Configuration","text":"<p>Each addon in the <code>cluster_addons</code> map supports the following configuration:</p> <ul> <li><code>create</code> - Whether to create the addon (default: <code>true</code>)</li> <li><code>name</code> - Addon name (defaults to map key)</li> <li><code>addon_version</code> - Version of the addon to install</li> <li><code>configuration_values</code> - JSON encoded configuration values</li> <li><code>most_recent</code> - Use the most recent version (default: <code>true</code>)</li> <li><code>preserve</code> - Preserve addon on delete (default: <code>false</code>)</li> <li><code>resolve_conflicts_on_create</code> - How to resolve conflicts on create (default: <code>\"OVERWRITE\"</code>)</li> <li><code>resolve_conflicts_on_update</code> - How to resolve conflicts on update (default: <code>\"OVERWRITE\"</code>)</li> <li><code>service_account_role_arn</code> - IAM role ARN for the addon's service account</li> <li><code>pod_identity_association</code> - Pod identity associations for the addon</li> <li><code>timeouts</code> - Timeout configuration for create/update/delete operations</li> <li><code>tags</code> - Additional tags for the addon</li> </ul>"},{"location":"modules/lacework/","title":"Lacework","text":""},{"location":"modules/lacework/#lacework","title":"Lacework","text":"<p>Deploy Lacework Agents</p> <pre><code>module \"lacework\" {\n  source  = \"tx-pts-dai/kubernetes-platform/aws//modules/datadog\"\n  version = ...\n\n  cluster_name = module.eks.cluster_name\n}\n</code></pre>"},{"location":"modules/lacework/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.10 aws &gt;= 5.0 kubernetes &gt;= 2.0.0 lacework &gt;= 2.0.0"},{"location":"modules/lacework/#providers","title":"Providers","text":"Name Version aws &gt;= 5.0 kubernetes &gt;= 2.0.0 lacework &gt;= 2.0.0"},{"location":"modules/lacework/#modules","title":"Modules","text":"Name Source Version lacework_k8s_datacollector lacework/agent/kubernetes 2.5.2"},{"location":"modules/lacework/#resources","title":"Resources","text":"Name Type kubernetes_namespace_v1.lacework resource lacework_agent_access_token.kubernetes resource aws_caller_identity.this data source"},{"location":"modules/lacework/#inputs","title":"Inputs","text":"Name Description Type Default Required agent_tags A map/dictionary of Tags to be assigned to the Lacework datacollector <code>map(string)</code> <code>{}</code> no cluster_name Name of the cluster <code>string</code> n/a yes enable_cluster_agent A boolean representing whether the Lacework cluster agent should be deployed <code>bool</code> <code>true</code> no namespace Namespace for Lacework resources <code>string</code> <code>\"lacework\"</code> no node_affinity Node affinity settings <pre>list(object({    key      = string    operator = string    values   = list(string)  }))</pre> <pre>[  {    \"key\": \"eks.amazonaws.com/compute-type\",    \"operator\": \"NotIn\",    \"values\": [      \"fargate\"    ]  }]</pre> no pod_priority_class_name Name of the pod priority class <code>string</code> <code>\"system-node-critical\"</code> no resources Resources for the Lacework agent <pre>object({    cpu_request = string    mem_request = string    cpu_limit   = string    mem_limit   = string  })</pre> <pre>{  \"cpu_limit\": \"1000m\",  \"cpu_request\": \"100m\",  \"mem_limit\": \"1024Mi\",  \"mem_request\": \"256Mi\"}</pre> no server_url Lacework server URL <code>string</code> <code>\"https://api.fra.lacework.net\"</code> no tolerations Tolerations for the Lacework agent <code>list(map(string))</code> <pre>[  {    \"effect\": \"NoSchedule\",    \"operator\": \"Exists\"  }]</pre> no"},{"location":"modules/lacework/#outputs","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/lacework/#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.0.0 kubernetes &gt;= 2.0.0 lacework &gt;= 1.18.2"},{"location":"modules/lacework/#providers_1","title":"Providers","text":"Name Version aws &gt;= 5.0.0 kubernetes &gt;= 2.0.0 lacework &gt;= 1.18.2"},{"location":"modules/lacework/#modules_1","title":"Modules","text":"Name Source Version lacework_k8s_datacollector lacework/agent/kubernetes 2.5.1"},{"location":"modules/lacework/#resources_1","title":"Resources","text":"Name Type kubernetes_namespace_v1.lacework resource lacework_agent_access_token.kubernetes resource aws_caller_identity.this data source"},{"location":"modules/lacework/#inputs_1","title":"Inputs","text":"Name Description Type Default Required agent_tags A map/dictionary of Tags to be assigned to the Lacework datacollector <code>map(string)</code> <code>{}</code> no cluster_name Name of the cluster <code>string</code> n/a yes enable_cluster_agent A boolean representing whether the Lacework cluster agent should be deployed <code>bool</code> <code>true</code> no namespace Namespace for Lacework resources <code>string</code> <code>\"lacework\"</code> no node_affinity Node affinity settings <pre>list(object({    key      = string    operator = string    values   = list(string)  }))</pre> <pre>[  {    \"key\": \"eks.amazonaws.com/compute-type\",    \"operator\": \"NotIn\",    \"values\": [      \"fargate\"    ]  }]</pre> no pod_priority_class_name Name of the pod priority class <code>string</code> <code>\"system-node-critical\"</code> no resources Resources for the Lacework agent <pre>object({    cpu_request = string    mem_request = string    cpu_limit   = string    mem_limit   = string  })</pre> <pre>{  \"cpu_limit\": \"1000m\",  \"cpu_request\": \"100m\",  \"mem_limit\": \"1024Mi\",  \"mem_request\": \"256Mi\"}</pre> no server_url Lacework server URL <code>string</code> <code>\"https://api.fra.lacework.net\"</code> no tolerations Tolerations for the Lacework agent <code>list(map(string))</code> <pre>[  {    \"effect\": \"NoSchedule\",    \"operator\": \"Exists\"  }]</pre> no"},{"location":"modules/lacework/#outputs_1","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/network/","title":"Network","text":""},{"location":"modules/network/#network-module","title":"Network Module","text":"<p>This module is a wrapper around the public VPC module with some additional configuration options suitable for the Tamedia platform.</p> <p>See the network example how to use it and how to retrieve informations on the created resources from another stack.</p>"},{"location":"modules/network/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.10 aws &gt;= 5.42"},{"location":"modules/network/#providers","title":"Providers","text":"Name Version aws &gt;= 5.42"},{"location":"modules/network/#modules","title":"Modules","text":"Name Source Version ssm ./../ssm n/a vpc terraform-aws-modules/vpc/aws 6.0.1"},{"location":"modules/network/#resources","title":"Resources","text":"Name Type aws_availability_zones.available data source"},{"location":"modules/network/#inputs","title":"Inputs","text":"Name Description Type Default Required az_count Number of availability zones to use <code>number</code> <code>3</code> no cidr The CIDR block for the VPC <code>string</code> <code>\"10.0.0.0/16\"</code> no create_vpc Create the VPC <code>bool</code> <code>true</code> no enable_nat_gateway Enable NAT Gateways <code>bool</code> <code>true</code> no single_nat_gateway Use a single NAT Gateway <code>bool</code> <code>true</code> no stack_name The stack name for the resources <code>string</code> n/a yes subnet_configs List of networks objects with their name and size in bits. The order of the list should not change. <code>list(map(number))</code> <pre>[  {    \"public\": 24  },  {    \"private\": 24  },  {    \"intra\": 26  },  {    \"database\": 26  },  {    \"redshift\": 26  },  {    \"karpenter\": 22  }]</pre> no tags A map of tags to add to all resources <code>map(string)</code> <code>{}</code> no"},{"location":"modules/network/#outputs","title":"Outputs","text":"Name Description cidr The base CIDR block for the VPC grouped_networks A map of subnet names to their respective details and list of CIDR blocks. network_cidr_blocks A map from network names to allocated address prefixes in CIDR notation. networks A list of network objects with name, az, hosts, and cidr_block. vpc Map of attributes for the VPC"},{"location":"modules/network/#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.42.0"},{"location":"modules/network/#providers_1","title":"Providers","text":"Name Version aws &gt;= 5.42.0"},{"location":"modules/network/#modules_1","title":"Modules","text":"Name Source Version vpc terraform-aws-modules/vpc/aws 5.8.1"},{"location":"modules/network/#resources_1","title":"Resources","text":"Name Type aws_availability_zones.available data source"},{"location":"modules/network/#inputs_1","title":"Inputs","text":"Name Description Type Default Required az_count Number of availability zones to use <code>number</code> <code>3</code> no cidr The CIDR block for the VPC <code>string</code> <code>\"10.0.0.0/16\"</code> no create_vpc Create the VPC <code>bool</code> <code>true</code> no enable_nat_gateway Enable NAT Gateways <code>bool</code> <code>true</code> no single_nat_gateway Use a single NAT Gateway <code>bool</code> <code>true</code> no stack_name The stack name for the resources <code>string</code> n/a yes subnet_configs List of networks objects with their name and size in bits. The order of the list should not change. <code>list(map(number))</code> <pre>[  {    \"public\": 24  },  {    \"private\": 24  },  {    \"intra\": 26  },  {    \"database\": 26  },  {    \"redshift\": 26  },  {    \"karpenter\": 22  }]</pre> no tags A map of tags to add to all resources <code>map(string)</code> <code>{}</code> no"},{"location":"modules/network/#outputs_1","title":"Outputs","text":"Name Description cidr The base CIDR block for the VPC grouped_networks A map of subnet names to their respective details and list of CIDR blocks. network_cidr_blocks A map from network names to allocated address prefixes in CIDR notation. networks A list of network objects with name, az, hosts, and cidr_block. vpc Map of attributes for the VPC"},{"location":"modules/security-group/","title":"Security group","text":""},{"location":"modules/security-group/#security-group-module","title":"Security Group Module","text":"<p>This module creates a security group and rules.</p>"},{"location":"modules/security-group/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.10 aws &gt;= 5.0"},{"location":"modules/security-group/#providers","title":"Providers","text":"Name Version aws &gt;= 5.0"},{"location":"modules/security-group/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/security-group/#resources","title":"Resources","text":"Name Type aws_security_group.this resource aws_security_group_rule.this resource"},{"location":"modules/security-group/#inputs","title":"Inputs","text":"Name Description Type Default Required create Create the security group. <code>bool</code> <code>true</code> no description The description of the security group. <code>string</code> <code>\"\"</code> no egress_rules The egress rules for the security group. <pre>map(object({    type                     = string    protocol                 = string    from_port                = number    to_port                  = number    description              = optional(string)    cidr_blocks              = optional(list(string))    ipv6_cidr_blocks         = optional(list(string))    prefix_list_ids          = optional(list(string))    self                     = optional(bool)    source_security_group_id = optional(string)  }))</pre> <code>{}</code> no ingress_rules The ingress rules for the security group. <pre>map(object({    type                     = string    protocol                 = string    from_port                = number    to_port                  = number    description              = optional(string)    cidr_blocks              = optional(list(string))    ipv6_cidr_blocks         = optional(list(string))    prefix_list_ids          = optional(list(string))    self                     = optional(bool)    source_security_group_id = optional(string)  }))</pre> <code>{}</code> no name The name of the security group, this name must be unique within the VPC. <code>string</code> n/a yes tags A map of tags to add to all resources. <code>map(string)</code> <code>{}</code> no vpc_id The VPC id to create the security group in. <code>string</code> n/a yes"},{"location":"modules/security-group/#outputs","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/ssm/","title":"Ssm","text":""},{"location":"modules/ssm/#ssm-parameter-store-for-terraform-outputs","title":"SSM Parameter Store for Terraform Outputs","text":""},{"location":"modules/ssm/#overview","title":"Overview","text":"<p>This Terraform module is designed to store Terraform outputs in AWS Systems Manager (SSM) Parameter Store and provide functionality to look up parameters for cross stack reference. It helps store and retrieve parameters across different terraform stacks and environments.</p>"},{"location":"modules/ssm/#features","title":"Features","text":"<ul> <li>Store parameters in SSM Parameter Store with customizable hierarchies.</li> <li>Retrieve parameters from SSM Parameter Store based on specified paths.</li> <li>Retrieve list of stack names and parameters.</li> <li>Filter stacks with prefixes.</li> <li>Support for securely storing sensitive parameters.</li> <li>Dynamically identify and retrieve the latest stack parameters.</li> </ul>"},{"location":"modules/ssm/#example","title":"Example","text":"<p>Store parameters in SSM Parameter Store: <pre><code>module \"ssm_parameters\" {\n  source           = \"./path-to-your-module\"\n\n  base_prefix      = \"infrastructure\"\n  stack_type       = \"platform\"\n  stack_name       = \"stack-123\"\n\n  parameters = {\n    cluster_endpoint = {\n      type           = \"String\"\n      insecure_value = \"https://cluster-zxcv.local\"\n    }\n    cluster_name = {\n      insecure_value = \"cluster-123\"\n    }\n  }\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n</code></pre></p> <p>Retrieve parameters from SSM Parameter Store: <pre><code>module \"ssm_lookup\" {\n  source           = \"./path-to-your-module\"\n\n  base_prefix       = \"infrastructure\"\n  stack_type        = \"platform\"\n  stack_name_prefix = \"stack-\"\n\n  lookup = [\n    \"cluster_endpoint\",\n    \"cluster_name\"\n  ]\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n</code></pre></p> <p>Outputs: <pre><code>ssm_lookup = {\n  \"filtered_parameters\" = {\n    \"/infrastructure/platform/stack-123/cluster_endpoint\" = \"cluster-zxcv\"\n    \"/infrastructure/platform/stack-123/cluster_name\" = \"foo\"\n    \"/infrastructure/platform/stack-234/cluster_endpoint\" = \"cluster-asjf\"\n    \"/infrastructure/platform/stack-234/cluster_name\" = \"bar\"\n  }\n  \"latest_stack_parameters\" = {\n    \"/infrastructure/platform/stack-234/cluster_endpoint\" = \"https://cluster-asjf.local\"\n    \"/infrastructure/platform/stack-234/cluster_name\" = \"bar\"\n  }\n  \"lookup\" = {\n    \"stack-123\" = {\n      \"cluster_endpoint\" = \"https://cluster-zxcv.local\"\n      \"cluster_name\" = \"foo\"\n    }\n    \"stack-234\" = {\n      \"cluster_endpoint\" = \"https://cluster-asjf.local\"\n      \"cluster_name\" = \"bar\"\n    }\n  }\n  # All parameters stored in SSM\n  \"parameters\" = tomap({\n    \"/infrastructure/platform/stack-123/cluster_endpoint\" = \"https://cluster-zxcv.local\"\n    \"/infrastructure/platform/stack-123/cluster_name\" = \"foo\"\n    \"/infrastructure/platform/stack-234/cluster_endpoint\" = \"https://cluster-asjf.local\"\n    \"/infrastructure/platform/stack-234/cluster_name\" = \"bar\"\n  })\n  \"stacks\" = tolist([\n    \"stack-123\",\n    \"stack-234\",\n  ])\n}\n</code></pre></p>"},{"location":"modules/ssm/#implementation-details","title":"Implementation Details","text":""},{"location":"modules/ssm/#storing-parameters","title":"Storing Parameters","text":"<p>Parameters are stored in SSM Parameter Store using the <code>aws_ssm_parameter</code> resource. The parameter name is constructed using the <code>base_prefix</code>, <code>stack_type</code>, and <code>stack_name</code>, forming a hierarchy.</p>"},{"location":"modules/ssm/#retrieving-parameters","title":"Retrieving Parameters","text":"<p>The module uses the <code>aws_ssm_parameters_by_path</code> data source to retrieve parameters from SSM Parameter Store based on the specified path. The retrieved parameters are processed to:</p> <ul> <li>Filter parameters by stack name prefix.</li> <li>Extract unique stack names.</li> <li>Create a lookup map for stack-specific parameters.</li> <li>Identify and retrieve the latest stack parameters.</li> </ul>"},{"location":"modules/ssm/#filtering-and-lookup","title":"Filtering and Lookup","text":"<p>The <code>filtered_parameters</code> local variable is used to filter parameters based on the stack name prefix. The <code>lookup</code> local variable creates a nested map of stack-specific parameters based on the provided lookup list. The <code>latest_stack_parameters</code> local variable identifies and retrieves parameters for the last created stack since we use timestamps in the stack names suffix.</p>"},{"location":"modules/ssm/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.10 aws &gt;= 5.42"},{"location":"modules/ssm/#providers","title":"Providers","text":"Name Version aws &gt;= 5.42"},{"location":"modules/ssm/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/ssm/#resources","title":"Resources","text":"Name Type aws_ssm_parameter.cluster_name resource aws_ssm_parameters_by_path.this data source"},{"location":"modules/ssm/#inputs","title":"Inputs","text":"Name Description Type Default Required base_prefix Base SSM namespace prefix for the parameters <code>string</code> <code>\"infrastructure\"</code> no create Create the SSM parameters <code>bool</code> <code>true</code> no lookup List of parameters to Lookup <code>list(any)</code> <code>[]</code> no parameters Map of SSM parameters to create <pre>map(object({    name           = optional(string)    type           = optional(string, \"String\")    value          = optional(string)    insecure_value = optional(string)  }))</pre> <code>{}</code> no stack_name The name of the stack <code>string</code> <code>null</code> no stack_name_prefix Filter all stacks that include this prefix in the name. <code>string</code> <code>\"\"</code> no stack_type The type of terraform stack to be used in the namespace prefix. platform, network, account, shared <code>string</code> <code>\"\"</code> no tags Default tags to apply to all resources <code>map(string)</code> <code>{}</code> no"},{"location":"modules/ssm/#outputs","title":"Outputs","text":"Name Description filtered_parameters List of parameters filtered by stack name prefix latest_stack_parameters Latest created stack parameters lookup Map of parameters from filtered parameters containing only keys defined in lookup parameters All parameters defined in SSM stacks List of stacks defined in SSM ordered by creation date (latest first)"},{"location":"modules/ssm/#contributions","title":"Contributions","text":"<p>Contributions to enhance the functionality and flexibility of this module are welcome. Please submit a pull request or open an issue to discuss any changes.</p>"},{"location":"modules/ssm/#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"}]}