{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#tamedia-kubernetes-as-a-service-kaas-terraform-module","title":"Tamedia Kubernetes as a Service (KaaS) Terraform Module","text":"<p>Opinionated batteries included Terraform module to deploy Kubernetes in AWS. Includes:</p> <p>Managed Addons:</p> <ul> <li>EBS CSI</li> <li>VPC CNI</li> <li>CoreDNS</li> <li>KubeProxy</li> </ul> <p>Core components (installed by default):</p> <ul> <li>Karpenter</li> <li>Metrics Server</li> <li>AWS Load Balancer Controller</li> <li>External DNS</li> <li>External Secrets Operator</li> <li>Prometheus Operator</li> <li>Grafana</li> <li>Fluent Operator</li> <li>Fluentbit for Fargate</li> <li>Reloader</li> </ul> <p>Additional components (optional):</p> <ul> <li>Cert Manager</li> <li>Ingress Nginx</li> <li>Downscaler</li> <li>ArgoCD</li> </ul> <p>Integrations (optional):</p> <ul> <li>Okta</li> <li>PagerDuty</li> <li>Slack</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<p>The module needs some resources to be deployed in order to operate correctly:</p> <p>IAM service-linked roles</p> <ul> <li>AWSServiceRoleForEC2Spot</li> <li>AWSServiceRoleForEC2SpotFleet</li> </ul>"},{"location":"#release-new-kubernetes-version","title":"Release new kubernetes version","text":"<p>important Each new kubernetes version needs it's own release. This is due to the fact that we should not skip kubernetes versions during a cluster upgrade.</p> <p>To release a new Kubernetes version, follow these steps:</p> <ol> <li>Update the version file:</li> <li>Open the <code>K8S_VERSION</code> file located in the root of the repository.</li> <li> <p>Update the version number to the next Kubernetes version.</p> </li> <li> <p>Commit the Changes:</p> </li> <li> <p>Commit the changes to the <code>K8S_VERSION</code> file with a meaningful commit message following the release proces. For example:      <pre><code>git add K8S_VERSION\ngit commit -m \"feat! update Kubernetes version to 1.30\"\n</code></pre></p> </li> <li> <p>Push the Changes:</p> </li> <li> <p>Push the changes to the main branch, the release workflow will automatically run. This workflow will:</p> <ul> <li>Read the updated Kubernetes version from the <code>K8S_VERSION</code> file.</li> <li>Determine the new module version based on the commit message.</li> <li>Create a new release with the updated module version and the kubernetes version as metadata. The format would be X.Y.Z+A.B where X.Y.Z is the module version and A.B is the kubenetes control plane version.</li> </ul> </li> <li> <p>Verify the Release:</p> </li> <li>Check the GitHub Actions page to ensure the release workflow completed successfully.</li> <li>Verify that the new module version is available in the Terraform Registry.</li> </ol>"},{"location":"#usage","title":"Usage","text":"<pre><code>module \"k8s_platform\" {\n  source = \"tx-pts-dai/kubernetes-platform/aws\"\n  # Pin this module to a specific version to avoid breaking changes\n  # version = \"0.0.0\"\n\n  name = \"example-platform\"\n\n  vpc = {\n    enabled = true\n  }\n\n  tags = {\n    Environment = \"sandbox\"\n    GithubRepo  = \"terraform-aws-kubernetes-platform\"\n  }\n}\n</code></pre> <p>See the Examples below for more use cases</p>"},{"location":"#explanation-and-description-of-interesting-use-cases","title":"Explanation and description of interesting use-cases","text":"<p>Why this module?</p> <ul> <li>To provide an AWS account with a K8s cluster with batteries included so that you can start deploying your workloads on a well-built foundation</li> <li>To encourage standardization and common practices</li> <li>To ease maintenance</li> </ul>"},{"location":"#reloader","title":"Reloader","text":"<p>The Stakater Reloader is a Kubernetes controller that automatically watches for changes in ConfigMaps and Secrets and triggers rolling restarts of the associated deployments, statefulsets, or daemonsets when these configurations are updated. This functionality ensures that applications deployed within a Kubernetes cluster always reflect the latest configuration without manual intervention.</p> <p>When an application relies on configuration data or sensitive information stored in ConfigMaps or Secrets, and these resources are modified, Reloader automates the process of applying these changes by updating the relevant pods. Without Reloader, such changes would require a manual pod restart or redeployment to take effect.</p> <p>Reloader is deployed by default on the cluster but is used as on demand via annotations.</p> <p>Considering this kubernetes deployment and the required annotation: <pre><code>kind: Deployment\nmetadata:\n  name: foo\n  annotations:\n    reloader.stakater.com/auto: \"true\"\nspec:\n  template:\n    metadata:\n</code></pre></p> <p>Reloader will now watch for updates and manage rolling restart of pods for this specific deployment.</p>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Complete - Includes creation of VPC, k8s cluster, addons and all the optional features.</li> <li>Datadog - EKS deployment with Datadog Operator integration</li> <li>Disable-Addons - EKS + Karpenter deployment with all addons disabled</li> <li>Lacework - EKS deployment with Lacework integration</li> <li>Network - VPC deployment with custom subnets for kubernetes</li> <li>Simple - Simplest EKS deployment with default VPC, addons, ... creation</li> </ul>"},{"location":"#cleanup-example-deployments","title":"Cleanup example deployments","text":"<p>Destroy Workflow - This manual workflow destroys deployed example deployments by selection the branch and the example to destroy.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>&lt; issues and contribution guidelines for public modules &gt;</p>"},{"location":"#pre-commit","title":"Pre-Commit","text":"<p>Installation: install pre-commit and execute <code>pre-commit install</code>. This will generate pre-commit hooks according to the config in <code>.pre-commit-config.yaml</code></p> <p>Before submitting a PR be sure to have used the pre-commit hooks or run: <code>pre-commit run -a</code></p> <p>The <code>pre-commit</code> command will run:</p> <ul> <li>Terraform fmt</li> <li>Terraform validate</li> <li>Terraform docs</li> <li>Terraform validate with tflint</li> <li>check for merge conflicts</li> <li>fix end of files</li> </ul> <p>as described in the <code>.pre-commit-config.yaml</code> file</p>"},{"location":"#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.7.0 aws &gt;= 5.42 helm &gt;= 2.12, &lt; 3.0.0 kubectl &gt;= 2.0.2 kubernetes &gt;= 2.27 time &gt;= 0.11"},{"location":"#providers","title":"Providers","text":"Name Version aws &gt;= 5.42 helm &gt;= 2.12, &lt; 3.0.0 time &gt;= 0.11"},{"location":"#modules","title":"Modules","text":"Name Source Version acm terraform-aws-modules/acm/aws 5.1.1 addons aws-ia/eks-blueprints-addons/aws 1.21.1 argocd ./modules/argocd n/a downscaler tx-pts-dai/downscaler/kubernetes 0.3.1 ebs_csi_driver_irsa terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks 5.55.0 eks terraform-aws-modules/eks/aws 20.37.1 karpenter terraform-aws-modules/eks/aws//modules/karpenter 20.36.0 karpenter_security_group ./modules/security-group n/a ssm ./modules/ssm n/a vpc_cni_irsa terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks 5.55.0"},{"location":"#resources","title":"Resources","text":"Name Type aws_route_table_association.karpenter resource aws_security_group_rule.eks_control_plane_ingress resource aws_subnet.karpenter resource helm_release.cluster_secret_store resource helm_release.karpenter_crd resource helm_release.karpenter_release resource helm_release.karpenter_resources resource helm_release.reloader resource time_sleep.wait_on_destroy resource time_static.timestamp_id resource aws_availability_zones.available data source aws_iam_roles.iam_cluster_admins data source aws_iam_roles.sso data source aws_region.current data source aws_route53_zone.base_domain_zone data source aws_route_tables.private_route_tables data source"},{"location":"#inputs","title":"Inputs","text":"Name Description Type Default Required acm_certificate ACM certificate configuration for the domain(s). Controls domain name, alternative domain names, wildcard configuration, and validation behavior.Options include:  - domain_name: Primary domain name for the certificate. If not provided, uses base_domain from other configuration.  - subject_alternative_names: List of additional domain names to include in the certificate.  - wildcard_certificates: When true, adds a wildcard prefix (*.) to all domains in the certificate.  - prepend_stack_id: When true, prepends the stack identifier to each domain name. Only works after random_string is created.  - wait_for_validation: When true, Terraform will wait for certificate validation to complete before proceeding. <pre>object({    domain_name               = optional(string)    subject_alternative_names = optional(list(string), [])    wildcard_certificates     = optional(bool, false)    prepend_stack_id          = optional(bool, false)    wait_for_validation       = optional(bool, false)  })</pre> <code>{}</code> no argocd Argo CD configurations <pre>object({    # Hub specific    enable_hub        = optional(bool, false)    namespace         = optional(string, \"argocd\")    hub_iam_role_name = optional(string, \"argocd-controller\")    helm_values = optional(list(string), [])    helm_set = optional(list(object({      name  = string      value = string    })), [])    # Spoke specific    enable_spoke = optional(bool, false)    hub_iam_role_arn  = optional(string, null)    hub_iam_role_arns = optional(list(string), null)    # Common    tags = optional(map(string), {})  })</pre> <code>{}</code> no aws_load_balancer_controller AWS Load Balancer Controller configurations <code>any</code> <code>{}</code> no base_domain Base domain for the platform, used for ingress and ACM certificates <code>string</code> <code>null</code> no cert_manager Cert Manager configurations <code>any</code> <code>{}</code> no cluster_admins Map of IAM roles to add as cluster admins. Only exact matching role names are returned <pre>map(object({    role_name         = string    kubernetes_groups = optional(list(string))  }))</pre> <code>{}</code> no create_addons Create the platform addons. if set to false, no addons will be created <code>bool</code> <code>true</code> no downscaler Downscaler configurations <code>any</code> <code>{}</code> no eks Map of EKS configurations <code>any</code> <code>{}</code> no enable_acm_certificate Enable ACM certificate <code>bool</code> <code>false</code> no enable_argocd Enable Argo CD <code>bool</code> <code>false</code> no enable_aws_load_balancer_controller Enable AWS Load Balancer Controller <code>bool</code> <code>true</code> no enable_cert_manager Enable Cert Manager <code>bool</code> <code>false</code> no enable_downscaler Enable Downscaler <code>bool</code> <code>false</code> no enable_external_dns Enable External DNS <code>bool</code> <code>true</code> no enable_external_secrets Enable External Secrets <code>bool</code> <code>true</code> no enable_fargate_fluentbit Enable Fargate Fluentbit <code>bool</code> <code>true</code> no enable_ingress_nginx Enable Ingress Nginx <code>bool</code> <code>false</code> no enable_metrics_server Enable Metrics Server <code>bool</code> <code>true</code> no enable_reloader Enable Reloader <code>bool</code> <code>true</code> no enable_sso_admin_auto_discovery Enable automatic discovery of SSO admin roles. When disabled, only explicitly defined cluster_admins are used. <code>bool</code> <code>true</code> no enable_timestamp_id Disable the timestamp-based ID generation. When true, uses a static ID instead of timestamp. <code>bool</code> <code>true</code> no external_dns External DNS configurations <code>any</code> <code>{}</code> no external_secrets External Secrets configurations <code>any</code> <code>{}</code> no fargate_fluentbit Fargate Fluentbit configurations <code>any</code> <code>{}</code> no ingress_nginx Ingress Nginx configurations <code>any</code> <code>{}</code> no karpenter Karpenter configurations <pre>object({    subnet_cidrs = optional(list(string), [])  })</pre> <code>{}</code> no karpenter_helm_set List of Karpenter Helm set values <pre>list(object({    name  = string    value = string  }))</pre> <code>[]</code> no karpenter_helm_values List of Karpenter Helm values <code>list(string)</code> <code>[]</code> no karpenter_resources_helm_set List of Karpenter Resources Helm set values <pre>list(object({    name  = string    value = string  }))</pre> <code>[]</code> no karpenter_resources_helm_values List of Karpenter Resources Helm values <code>list(string)</code> <code>[]</code> no metrics_server Metrics Server configurations <code>any</code> <code>{}</code> no name The name of the platform, a timestamp will be appended to this name to make the stack_name. If not provided, the name of the directory will be used. <code>string</code> <code>\"\"</code> no reloader Reloader configurations <code>any</code> <code>{}</code> no tags Default tags to apply to all resources <code>map(string)</code> <code>{}</code> no vpc VPC configurations <pre>object({    vpc_id          = optional(string)    vpc_cidr        = optional(string)    private_subnets = optional(list(string))    intra_subnets   = optional(list(string))  })</pre> <code>{}</code> no"},{"location":"#outputs","title":"Outputs","text":"Name Description argocd Map of attributes for the ArgoCD module eks Map of attributes for the EKS cluster karpenter Map of attributes for the Karpenter module"},{"location":"#authors","title":"Authors","text":"<p>Module is maintained by Alfredo Gottardo, David Beauvererd, Davide Cammarata, Francisco Ferreira,  Roland Bapst and Samuel Wibrow</p>"},{"location":"#license","title":"License","text":"<p>Apache 2 Licensed. See LICENSE for full details.</p>"},{"location":"faq/","title":"FAQ","text":"<p>tba</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This getting started guide will help you deploy your first EKS cluster.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools locally:</p> <ul> <li>awscli</li> <li>kubectl</li> <li>terraform</li> </ul>"},{"location":"getting-started/#deploy","title":"Deploy","text":"<ol> <li> <p>For consuming EKS Blueprints, please see the Consumption section. For exploring and trying out the patterns provided, please clone the project locally to quickly get up and running with a pattern. After cloning the project locally, <code>cd</code> into the pattern directory of your choice.</p> </li> <li> <p>To provision the pattern, the typical steps of execution are as follows:</p> <pre><code>terraform init\nterraform apply -auto-approve\n</code></pre> <p>For patterns that deviate from this general flow, see the pattern's respective <code>README.md</code> for more details.</p> <p>Terraform targeted apply</p> <p>Please see the Terraform Caveats section for details on the use of targeted Terraform apply's</p> </li> <li> <p>Once all of the resources have successfully been provisioned, the following command can be used to update the <code>kubeconfig</code> on your local machine and allow you to interact with your EKS Cluster using <code>kubectl</code>.</p> <pre><code>aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt; --alias &lt;CLUSTER_NAME&gt;\n</code></pre> <p>Pattern Terraform outputs</p> <p>Most examples will output the <code>aws eks update-kubeconfig ...</code> command as part of the Terraform apply output to simplify this process for users</p> <p>Private clusters</p> <p>Clusters that do not enable the clusters public endpoint will require users to access the cluster from within the VPC. For these patterns, a sample EC2 or other means are provided to demonstrate how to access those clusters privately</p> <p>and without exposing the public endpoint. Please see the respective pattern's <code>README.md</code> for more details.</p> </li> <li> <p>Once you have updated your <code>kubeconfig</code>, you can verify that you are able to interact with your cluster by running the following command:</p> <pre><code>kubectl get nodes\n</code></pre> <p>This should return a list of the node(s) running in the cluster created. If any errors are encountered, please re-trace the steps above and consult the pattern's <code>README.md</code> for more details on any additional/specific steps that may be required.</p> </li> </ol>"},{"location":"getting-started/#destroy","title":"Destroy","text":"<p>To teardown and remove the resources created in the pattern, the typical steps of execution are as follows:</p> <pre><code>terraform destroy -auto-approve\n</code></pre> <p>Resources created outside of Terraform</p> <p>Depending on the pattern, some resources may have been created that Terraform is not aware of that will cause issues when attempting to clean up the pattern. For example, Karpenter is responsible for creating additional EC2 instances to satisfy the pod scheduling requirements. These instances will not be cleaned up by Terraform and will need to be de-provisioned BEFORE attempting to <code>terraform destroy</code>. This is why it is important that the addons, or any resources provisioned onto the cluster are cleaned up first. Please see the respective pattern's <code>README.md</code> for more details.</p>"},{"location":"examples/argocd/","title":"Argocd","text":"<pre><code>terraform {\n  required_version = \"&gt;= 1.11\"\n\n  backend \"s3\" {\n    bucket               = \"tf-state-911453050078\"\n    key                  = \"modules/argocd/examples/simple.tfstate\"\n    workspace_key_prefix = \"terraform-aws-kubernetes-platform\"\n    region               = \"eu-central-1\"\n    use_lockfile         = true\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"~&gt; 2.6\"\n    }\n    kubectl = {\n      source  = \"alekc/kubectl\"\n      version = \"~&gt; 2.0\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~&gt; 2.27\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = local.region\n}\n\nprovider \"kubernetes\" {\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nprovider \"helm\" {\n  kubernetes {\n    host                   = module.k8s_platform.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n    exec {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      command     = \"aws\"\n      args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n    }\n  }\n}\n\nlocals {\n  region = \"eu-central-1\"\n}\n\nmodule \"k8s_platform\" {\n  source = \"./../../..\"\n\n  name = \"ex-argocd\"\n\n  cluster_admins = {\n    cicd = {\n      role_name = \"cicd-iac\"\n    }\n  }\n\n  tags = {\n    Environment = \"sandbox\"\n    GithubRepo  = \"terraform-aws-kubernetes-platform\"\n    GithubOrg   = \"tx-pts-dai\"\n  }\n\n  vpc = {\n    enabled = true\n    cidr    = \"10.0.0.0/16\"\n    max_az  = 3\n    subnet_configs = [\n      { public = 24 },\n      { private = 24 },\n      { intra = 26 },\n    ]\n  }\n\n  create_addons = false\n}\n\nmodule \"hub\" {\n  source = \"./..\"\n\n  enable_hub = true\n\n  cluster_name = module.k8s_platform.eks.cluster_name\n}\n\nmodule \"spoke\" {\n  source = \"./..\"\n\n  enable_spoke = true\n\n  cluster_name = module.k8s_platform.eks.cluster_name\n\n  hub_iam_role_arn = module.hub.hub_iam_role_arn\n\n  hub_iam_role_arns = [\"arn:aws:iam::123456789012:role/another-role\"]\n}\n</code></pre>"},{"location":"examples/complete/","title":"Complete","text":"<pre><code>terraform {\n  required_version = \"&gt;= 1.7.0\"\n\n  backend \"s3\" {\n    bucket               = \"tf-state-911453050078\"\n    key                  = \"examples/complete.tfstate\"\n    workspace_key_prefix = \"terraform-aws-kubernetes-platform\"\n    use_lockfile         = true\n    region               = \"eu-central-1\"\n    encrypt              = true\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"~&gt; 2.6\"\n    }\n    kubectl = {\n      source  = \"alekc/kubectl\"\n      version = \"~&gt; 2.0\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~&gt; 2.27\"\n    }\n    cloudflare = {\n      source  = \"cloudflare/cloudflare\"\n      version = \"~&gt; 4.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = local.region\n}\n\nprovider \"kubernetes\" {\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nprovider \"helm\" {\n  kubernetes {\n    host                   = module.k8s_platform.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n    exec {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      command     = \"aws\"\n      args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n    }\n  }\n}\n\nprovider \"kubectl\" {\n  apply_retry_count      = 5\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  load_config_file       = false\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nlocals {\n  region = \"eu-central-1\"\n}\n\ndata \"aws_vpc\" \"default\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"central\"]\n  }\n}\n\ndata \"aws_subnets\" \"private_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*private*\"]\n  }\n}\n\ndata \"aws_subnets\" \"intra_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*intra*\"]\n  }\n}\n\nmodule \"k8s_platform\" {\n  source = \"../../\"\n\n  name = \"ex-complete\"\n\n  cluster_admins = {\n    cicd = {\n      role_name = \"cicd-iac\"\n    }\n  }\n\n  tags = {\n    Environment = \"sandbox\"\n    GithubRepo  = \"terraform-aws-kubernetes-platform\"\n    GithubOrg   = \"tx-pts-dai\"\n  }\n\n  vpc = {\n    vpc_id          = data.aws_vpc.default.id\n    vpc_cidr        = data.aws_vpc.default.cidr_block\n    private_subnets = data.aws_subnets.private_subnets.ids\n    intra_subnets   = data.aws_subnets.intra_subnets.ids\n  }\n\n  karpenter_resources_helm_set = [\n    {\n      name  = \"global.eksDiscovery.tags.subnets.karpenter\\\\.sh/discovery\"\n      value = \"shared\"\n    }\n  ]\n\n  enable_downscaler = true\n\n}\ndata \"aws_secretsmanager_secret_version\" \"cloudflare\" {\n  secret_id = \"dai/cloudflare/tamedia/apiToken\"\n}\n\nprovider \"cloudflare\" {\n  api_token = jsondecode(data.aws_secretsmanager_secret_version.cloudflare.secret_string)[\"apiToken\"]\n}\n\nlocals {\n  zones = {\n    \"kaas-example.tamedia.tech\" = {\n      comment = \"DAI KaaS example complete\"\n    }\n  }\n}\n\n# Manage DNS sub-domains in cloudflare and attach them to they parent in route53\nmodule \"cloudflare\" {\n  source = \"../../modules/cloudflare\"\n\n  for_each = local.zones\n\n  zone_name    = module.route53_zones[each.key].route53_zone_name[each.key]\n  comment      = \"Managed by KAAS examples\"\n  name_servers = [for i in range(4) : module.route53_zones[each.key].route53_zone_name_servers[each.key][i]]\n  account_id   = jsondecode(data.aws_secretsmanager_secret_version.cloudflare.secret_string)[\"accountId\"]\n}\n\nmodule \"route53_zones\" {\n  source  = \"terraform-aws-modules/route53/aws//modules/zones\"\n  version = \"5.0.0\"\n\n  for_each = local.zones\n\n  zones = {\n    (each.key) = {\n      comment = each.value.comment\n    }\n  }\n}\noutput \"eks\" {\n  description = \"eks module outputs\"\n  value       = module.k8s_platform.eks\n}\n\noutput \"zconfigure_kubectl\" {\n  description = \"Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig\"\n  value       = \"aws eks --region ${local.region} update-kubeconfig --name ${module.k8s_platform.eks.cluster_name} --alias ${module.k8s_platform.eks.cluster_name}\"\n}\n\noutput \"karpenter\" {\n  description = \"karpenter module outputs\"\n  value       = module.k8s_platform.karpenter\n}\n</code></pre>"},{"location":"examples/datadog/","title":"Datadog","text":"<pre><code>terraform {\n  required_version = \"&gt;= 1.11\"\n\n  backend \"s3\" {\n    bucket               = \"tf-state-911453050078\"\n    key                  = \"examples/datadog.tfstate\"\n    workspace_key_prefix = \"terraform-aws-kubernetes-platform\"\n    use_lockfile         = true\n    region               = \"eu-central-1\"\n    encrypt              = true\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~&gt; 2.27\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"~&gt; 2.6\"\n    }\n    kubectl = {\n      source  = \"alekc/kubectl\"\n      version = \"~&gt; 2.0\"\n    }\n    time = {\n      source  = \"hashicorp/time\"\n      version = \"~&gt; 0.11\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = local.region\n}\n\n\nprovider \"kubernetes\" {\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nprovider \"helm\" {\n  kubernetes {\n    host                   = module.k8s_platform.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n    exec {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      command     = \"aws\"\n      args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n    }\n  }\n}\n\nprovider \"kubectl\" {\n  apply_retry_count      = 5\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  load_config_file       = false\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nlocals {\n  region = \"eu-central-1\"\n}\n\ndata \"aws_vpc\" \"default\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"central\"]\n  }\n}\n\ndata \"aws_subnets\" \"private_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*private*\"]\n  }\n}\n\ndata \"aws_subnets\" \"intra_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*intra*\"]\n  }\n}\n\nmodule \"k8s_platform\" {\n  source = \"../../\"\n\n  name = \"ex-datadog\"\n\n  cluster_admins = {\n    cicd = {\n      role_name = \"cicd-iac\"\n    }\n  }\n\n  vpc = {\n    vpc_id          = data.aws_vpc.default.id\n    vpc_cidr        = data.aws_vpc.default.cidr_block\n    private_subnets = data.aws_subnets.private_subnets.ids\n    intra_subnets   = data.aws_subnets.intra_subnets.ids\n  }\n\n  karpenter_helm_values = [\n    &lt;&lt;-YAML\n      podAnnotations:\n        \"ad.datadoghq.com/controller.checks\": |\n          karpenter:\n            init_config: {}\n            instances:\n              - openmetrics_endpoint: http://%%host%%:8080/metrics\n      controller:\n        resources:\n          requests:\n            cpu: 0.5\n            memory: \"768Mi\"\n      YAML\n  ]\n\n  karpenter_resources_helm_set = [\n    {\n      name  = \"global.eksDiscovery.tags.subnets.karpenter\\\\.sh/discovery\"\n      value = \"shared\"\n    }\n  ]\n\n  tags = {\n    Environment = \"sandbox\"\n    GithubRepo  = \"terraform-aws-kubernetes-platform\"\n    GithubOrg   = \"tx-pts-dai\"\n  }\n}\n\nmodule \"datadog\" {\n  source = \"../../modules/datadog\"\n\n  cluster_name   = module.k8s_platform.eks.cluster_name\n  datadog_secret = \"dai/datadog/tamedia/keys\"\n  environment    = \"sandbox\"\n  product_name   = \"dai\"\n\n  datadog_operator_helm_values = [\n    &lt;&lt;-YAML\n    remoteConfiguration:\n      enabled: true\n    YAML\n  ]\n\n\n  datadog_operator_helm_set = [\n    {\n      name  = \"replicas\"\n      value = 2\n    }\n  ]\n\n  # Example: how to override specs in the Datadog Custom Resource\n  # How to update the number of clusterAgent replicas and enable datadog agent as external metrics server\n  datadog_agent_helm_values = [\n    &lt;&lt;-YAML\n    spec:\n      features:\n        externalMetricsServer:\n          enabled: true\n          useDatadogMetrics: true\n      override:\n        clusterAgent:\n          replicas: 1\n    YAML\n  ]\n\n  datadog_agent_helm_set = [\n    {\n      name  = \"spec.features.admissionController.agentSidecarInjection.image.tag\",\n      value = \"7.57.2\"\n    }\n  ]\n\n  depends_on = [module.k8s_platform]\n}\noutput \"eks\" {\n  description = \"eks module outputs\"\n  value       = module.k8s_platform.eks\n}\n\noutput \"zconfigure_kubectl\" {\n  description = \"Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig\"\n  value       = \"aws eks --region ${local.region} update-kubeconfig --name ${module.k8s_platform.eks.cluster_name} --alias ${module.k8s_platform.eks.cluster_name}\"\n}\n</code></pre>"},{"location":"examples/lacework/","title":"Lacework","text":"<pre><code>terraform {\n  required_version = \"&gt;= 1.7.0\"\n\n  backend \"s3\" {\n    bucket               = \"tf-state-911453050078\"\n    key                  = \"examples/lacework.tfstate\"\n    workspace_key_prefix = \"terraform-aws-kubernetes-platform\"\n    use_lockfile         = true\n    region               = \"eu-central-1\"\n    encrypt              = true\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~&gt; 2.27\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"~&gt; 2.6\"\n    }\n    kubectl = {\n      source  = \"alekc/kubectl\"\n      version = \"~&gt; 2.0\"\n    }\n    lacework = {\n      source  = \"lacework/lacework\"\n      version = \"~&gt; 2.0\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = local.region\n}\n\nprovider \"kubernetes\" {\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nprovider \"helm\" {\n  kubernetes {\n    host                   = module.k8s_platform.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n    exec {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      command     = \"aws\"\n      args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n    }\n  }\n}\n\nprovider \"kubectl\" {\n  apply_retry_count      = 5\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  load_config_file       = false\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\ndata \"aws_secretsmanager_secret\" \"lacework\" {\n  name = \"dai/lacework/tamedia/apiKey\"\n}\n\ndata \"aws_secretsmanager_secret_version\" \"lacework\" {\n  secret_id = data.aws_secretsmanager_secret.lacework.id\n}\n\nprovider \"lacework\" {\n  account    = jsondecode(data.aws_secretsmanager_secret_version.lacework.secret_string)[\"account\"]\n  subaccount = jsondecode(data.aws_secretsmanager_secret_version.lacework.secret_string)[\"subAccount\"]\n  api_key    = jsondecode(data.aws_secretsmanager_secret_version.lacework.secret_string)[\"keyId\"]\n  api_secret = jsondecode(data.aws_secretsmanager_secret_version.lacework.secret_string)[\"secret\"]\n}\n\nlocals {\n  region = \"eu-central-1\"\n}\n\ndata \"aws_vpc\" \"default\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"central\"]\n  }\n}\n\ndata \"aws_subnets\" \"private_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*private*\"]\n  }\n}\n\ndata \"aws_subnets\" \"intra_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*intra*\"]\n  }\n}\n\nmodule \"k8s_platform\" {\n  source = \"../../\"\n\n  name = \"ex-lacework\"\n\n  cluster_admins = {\n    cicd = {\n      role_name = \"cicd-iac\"\n    }\n  }\n\n  vpc = {\n    vpc_id          = data.aws_vpc.default.id\n    vpc_cidr        = data.aws_vpc.default.cidr_block\n    private_subnets = data.aws_subnets.private_subnets.ids\n    intra_subnets   = data.aws_subnets.intra_subnets.ids\n  }\n\n  karpenter_resources_helm_set = [\n    {\n      name  = \"global.eksDiscovery.tags.subnets.karpenter\\\\.sh/discovery\"\n      value = \"shared\"\n    }\n  ]\n\n  tags = {\n    Environment = \"sandbox\"\n    GithubRepo  = \"terraform-aws-kubernetes-platform\"\n    GithubOrg   = \"tx-pts-dai\"\n  }\n\n}\n\nmodule \"lacework\" {\n  source = \"../../modules/lacework\"\n\n  cluster_name = module.k8s_platform.eks.cluster_name\n\n  agent_tags = {\n    KubernetesCluster = module.k8s_platform.eks.cluster_name\n  }\n}\noutput \"eks\" {\n  description = \"eks module outputs\"\n  value       = module.k8s_platform.eks\n}\n\noutput \"zconfigure_kubectl\" {\n  description = \"Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig\"\n  value       = \"aws eks --region ${local.region} update-kubeconfig --name ${module.k8s_platform.eks.cluster_name} --alias ${module.k8s_platform.eks.cluster_name}\"\n}\n</code></pre>"},{"location":"examples/simple/","title":"Simple","text":"<pre><code>terraform {\n  required_version = \"&gt;= 1.7.0\"\n\n  backend \"s3\" {\n    bucket               = \"tf-state-911453050078\"\n    key                  = \"examples/simple.tfstate\"\n    workspace_key_prefix = \"terraform-aws-kubernetes-platform\"\n    use_lockfile         = true\n    region               = \"eu-central-1\"\n    encrypt              = true\n  }\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~&gt; 5.0\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \"~&gt; 2.6\"\n    }\n    kubectl = {\n      source  = \"alekc/kubectl\"\n      version = \"~&gt; 2.0\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \"~&gt; 2.27\"\n    }\n  }\n}\n\nprovider \"aws\" {\n  region = local.region\n}\n\nprovider \"kubernetes\" {\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nprovider \"helm\" {\n  kubernetes {\n    host                   = module.k8s_platform.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n    exec {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      command     = \"aws\"\n      args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n    }\n  }\n}\n\nprovider \"kubectl\" {\n  apply_retry_count      = 5\n  host                   = module.k8s_platform.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.k8s_platform.eks.cluster_certificate_authority_data)\n  load_config_file       = false\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    args        = [\"eks\", \"get-token\", \"--cluster-name\", module.k8s_platform.eks.cluster_name]\n  }\n}\n\nlocals {\n  region = \"eu-central-1\"\n}\n\ndata \"aws_vpc\" \"default\" {\n  filter {\n    name   = \"tag:Name\"\n    values = [\"central\"]\n  }\n}\n\ndata \"aws_subnets\" \"private_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*private*\"]\n  }\n}\n\ndata \"aws_subnets\" \"intra_subnets\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n\n  filter {\n    name   = \"tag:Name\"\n    values = [\"*intra*\"]\n  }\n}\n\nmodule \"k8s_platform\" {\n  source = \"../../\"\n\n  name = \"ex-simple\"\n\n  cluster_admins = {\n    cicd = {\n      role_name = \"cicd-iac\"\n    }\n  }\n\n  vpc = {\n    vpc_id          = data.aws_vpc.default.id\n    vpc_cidr        = data.aws_vpc.default.cidr_block\n    private_subnets = data.aws_subnets.private_subnets.ids\n    intra_subnets   = data.aws_subnets.intra_subnets.ids\n  }\n\n  karpenter_resources_helm_set = [\n    {\n      name  = \"global.eksDiscovery.tags.subnets.karpenter\\\\.sh/discovery\"\n      value = \"shared\"\n    }\n  ]\n\n  tags = {\n    Environment = \"sandbox\"\n    GithubRepo  = \"terraform-aws-kubernetes-platform\"\n    GithubOrg   = \"tx-pts-dai\"\n  }\n}\noutput \"eks\" {\n  description = \"eks module outputs\"\n  value       = module.k8s_platform.eks\n}\n\noutput \"zconfigure_kubectl\" {\n  description = \"Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig\"\n  value       = \"aws eks --region ${local.region} update-kubeconfig --name ${module.k8s_platform.eks.cluster_name} --alias ${module.k8s_platform.eks.cluster_name}\"\n}\n</code></pre>"},{"location":"examples/snippets/karpenter/","title":"Karpenter","text":"<p>By default the Kubernetes as a Service (KaaS) module will deploy a Karpenter controller and the custom resources for creating nodes. These resources are NodePool and Ec2NodeClass. You can think of these as NodeClass defines the ec2 launch template and NodePool defines the node group. The Karpenter controller will then create the nodes based on the node class and node pool.</p>"},{"location":"examples/snippets/karpenter/#configuration","title":"Configuration","text":"<p>To customise the Karpenter configuration, you need to define the necessary variables in the <code>kaas</code> module. The following example shows how to do this:</p> <pre><code>module \"k8s_platform\" {\n  source = \"../../\"\n\n  name = \"ex-complete\"\n\n  ...redacted for brevity...\n\n  # Karpenter general configuration variable\n  karpenter = {\n    values = [\n      # Add your custom values here\n      # These will be treated like how helm treat values inputs. eg. helm install -f values.yaml -f values2.yaml\n      replicas: 2\n    ]\n    # Karpenter Helm chart sets\n    set = [\n      {\n        name  = \"replicas\"\n        value = 1\n      }\n    ]\n\n\n## Karpenter Custom Resources\n\nThe Karpenter custom resources are defined in the `karpenter_resources` variable. This variable is a map of values that will be passed to the [Karpenter resources Helm chart](https://github.com/DND-IT/helm-charts/tree/main/charts/karpenter-resources#karpenter-resources). The following example shows how to define the Karpenter custom resources:\n\n```hcl\nmodule \"k8s_platform\" {\n  source = \"../../\"\n\n  name = \"ex-complete\"\n\n  ...redacted for brevity...\n\n  # Karpenter general configuration variable\n  karpenter = {\n    # Karpenter custom resources overrides\n    # Direct inline overrides to the Karpenter custom resources\n    karpenter_resources = {\n      values = [\n        file(\"${path.module}/karpenter-values.yaml\"), # Set the values file\n        &lt;&lt;-EOT # Inline YAML\n        nodePools:\n          default:\n            requirements:\n              - key: karpenter.k8s.aws/instance-category\n                operator: In\n                values: [\"t\"]\n        EOT\n      ]\n      set = [ # Set the values directly\n        {\n          name  = \"nodePools.default.requirements[0].key\"\n          value = \"karpenter.k8s.aws/instance-category\"\n        },\n        {\n          name  = \"nodePools.default.requirements[0].operator\"\n          value = \"In\"\n        },\n        {\n          name  = \"nodePools.default.requirements[0].values\"\n          value = \"[\\\"t\\\"]\"\n        }\n      ]\n    }\n  }\n}\n</code></pre> <pre><code>data \"helm_template\" \"karpenter_custom_resources\" {\n  name       = \"karpenter-resources\"\n  chart      = \"karpenter-resources\"\n  version    = \"0.3.1\"\n  repository = \"https://dnd-it.github.io/helm-charts\"\n  namespace  = local.karpenter.namespace\n\n  values = [\n    &lt;&lt;-EOT\n    global:\n      role: ${module.k8s_platform.karpenter.node_iam_role_name}\n      eksDiscovery:\n        enabled: true\n        clusterName: ${module.k8s_platform.eks.cluster_name}\n\n    nodePools:\n      custom:\n        requirements:\n          - key: karpenter.k8s.aws/instance-category\n            operator: In\n            values: [\"t\"]\n        labels:\n          my-custom-nodepool: \"true\"\n        taints:\n          - key: \"karpenter.sh/capacity-type\"\n            value: \"spot\"\n            effect: \"NoSchedule\"\n        limits:\n          resources:\n            cpu: 100\n            memory: 10Gi\n        providerRef:\n          name: custom\n\n    ec2NodeClasses:\n      custom:\n        amiFamily: al2023\n        amiSelectorTerms:\n          - alias: al2023@v20240807\n        tags:\n          Name: \"custom-node-class\"\n        kubelet:\n          maxPods: 10\n    EOT\n  ]\n}\n</code></pre>"},{"location":"modules/addon/","title":"Addon","text":""},{"location":"modules/addon/#helm-release-and-irsa-integration-deprecated","title":"Helm release and IRSA integration [DEPRECATED]","text":"<p>This module is deprecated and will be removed in a future release. EKS Pod Identity Agent (PIA) will be used instead or IRSA thus removing the need for this module.</p> <p>This module deploys a Helm release and configures the service account to use an IAM role for service accounts (IRSA).</p> <p>This is a copy of the EKS Blueprints addon module, with some opinionated defaults.</p>"},{"location":"modules/addon/#usage","title":"Usage","text":""},{"location":"modules/addon/#create-addon-helm-release-w-iam-role-for-service-account-irsa","title":"Create Addon (Helm Release) w/ IAM Role for Service Account (IRSA)","text":"<pre><code>module \"eks_blueprints_addon\" {\n  source = \"aws-ia/eks-blueprints-addon/aws\"\n  version = \"~&gt; 1.0\" #ensure to update this to the latest/desired version\n\n  chart            = \"karpenter\"\n  chart_version    = \"0.16.2\"\n  repository       = \"https://charts.karpenter.sh/\"\n  description      = \"Kubernetes Node Autoscaling: built for flexibility, performance, and simplicity\"\n  namespace        = \"karpenter\"\n  create_namespace = true\n\n  set = [\n    {\n      name  = \"clusterName\"\n      value = \"eks-blueprints-addon-example\"\n    },\n    {\n      name  = \"clusterEndpoint\"\n      value = \"https://EXAMPLED539D4633E53DE1B71EXAMPLE.gr7.us-west-2.eks.amazonaws.com\"\n    },\n    {\n      name  = \"aws.defaultInstanceProfile\"\n      value = \"arn:aws:iam::111111111111:instance-profile/KarpenterNodeInstanceProfile-complete\"\n    }\n  ]\n\n  set_irsa_names = [\"serviceAccount.annotations.eks\\\\.amazonaws\\\\.com/role-arn\"]\n  # # Equivalent to the following but the ARN is only known internally to the module\n  # set = [{\n  #   name  = \"serviceAccount.annotations.eks\\\\.amazonaws\\\\.com/role-arn\"\n  #   value = iam_role_arn.this[0].arn\n  # }]\n\n  # IAM role for service account (IRSA)\n  create_role = true\n  role_name   = \"karpenter-controller\"\n  role_policies = {\n    karpenter = \"arn:aws:iam::111111111111:policy/Karpenter_Controller_Policy-20221008165117447500000007\"\n  }\n\n  oidc_providers = {\n    this = {\n      provider_arn = \"oidc.eks.us-west-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE\"\n      # namespace is inherited from chart\n      service_account = \"karpenter\"\n    }\n  }\n\n  tags = {\n    Environment = \"dev\"\n  }\n}\n</code></pre>"},{"location":"modules/addon/#create-addon-helm-release-only","title":"Create Addon (Helm Release) Only","text":"<pre><code>module \"eks_blueprints_addon\" {\n  source = \"aws-ia/eks-blueprints-addon/aws\"\n  version = \"~&gt; 1.0\" #ensure to update this to the latest/desired version\n\n  chart         = \"metrics-server\"\n  chart_version = \"3.8.2\"\n  repository    = \"https://kubernetes-sigs.github.io/metrics-server/\"\n  description   = \"Metric server helm Chart deployment configuration\"\n  namespace     = \"kube-system\"\n\n  values = [\n    &lt;&lt;-EOT\n      podDisruptionBudget:\n        maxUnavailable: 1\n      metrics:\n        enabled: true\n    EOT\n  ]\n\n  set = [\n    {\n      name  = \"replicas\"\n      value = 3\n    }\n  ]\n}\n</code></pre>"},{"location":"modules/addon/#create-iam-role-for-service-account-irsa-only","title":"Create IAM Role for Service Account (IRSA) Only","text":"<pre><code>module \"eks_blueprints_addon\" {\n  source = \"aws-ia/eks-blueprints-addon/aws\"\n  version = \"~&gt; 1.0\" #ensure to update this to the latest/desired version\n\n  # Disable helm release\n  create_release = false\n\n  # IAM role for service account (IRSA)\n  create_role = true\n  create_policy = false\n  role_name   = \"aws-vpc-cni-ipv4\"\n  role_policies = {\n    AmazonEKS_CNI_Policy = \"arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\"\n  }\n\n  oidc_providers = {\n    this = {\n      provider_arn    = \"oidc.eks.us-west-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B71EXAMPLE\"\n      namespace       = \"kube-system\"\n      service_account = \"aws-node\"\n    }\n  }\n\n  tags = {\n    Environment = \"dev\"\n  }\n}\n</code></pre>"},{"location":"modules/addon/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.0 aws &gt;= 4.47 helm &gt;= 2.9, &lt; 3.0.0 time &gt;= 0.11"},{"location":"modules/addon/#providers","title":"Providers","text":"Name Version aws &gt;= 4.47 helm &gt;= 2.9, &lt; 3.0.0 time &gt;= 0.11"},{"location":"modules/addon/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/addon/#resources","title":"Resources","text":"Name Type aws_iam_policy.this resource aws_iam_role.this resource aws_iam_role_policy_attachment.additional resource aws_iam_role_policy_attachment.this resource helm_release.additional resource helm_release.this resource time_sleep.additional resource time_sleep.this resource aws_caller_identity.current data source aws_iam_policy_document.assume data source aws_iam_policy_document.this data source aws_partition.current data source"},{"location":"modules/addon/#inputs","title":"Inputs","text":"Name Description Type Default Required additional_custom_delay_triggers List of resources to trigger the delay of additional helm releases <code>list(string)</code> <code>[]</code> no additional_delay_create_duration The duration to wait before creating additional helm releases. 30s, 1m, etc. <code>string</code> <code>null</code> no additional_delay_destroy_duration The duration to wait before destroying additional helm releases. 30s, 1m, etc. <code>string</code> <code>null</code> no additional_helm_releases A map of Helm releases to create. This provides the ability to pass in an arbitrary map of Helm chart definitions to create <code>any</code> <code>{}</code> no allow_self_assume_role Determines whether to allow the role to be assume itself <code>bool</code> <code>false</code> no assume_role_condition_test Name of the IAM condition operator to evaluate when assuming the role <code>string</code> <code>\"StringEquals\"</code> no atomic If set, installation process purges chart on fail. The wait flag will be set automatically if atomic is used. Defaults to <code>false</code> <code>bool</code> <code>null</code> no chart Chart name to be installed. The chart name can be local path, a URL to a chart, or the name of the chart if <code>repository</code> is specified <code>string</code> <code>\"\"</code> no chart_version Specify the exact chart version to install. If this is not specified, the latest version is installed <code>string</code> <code>null</code> no cleanup_on_fail Allow deletion of new resources created in this upgrade when upgrade fails. Defaults to <code>false</code> <code>bool</code> <code>null</code> no create Controls if resources should be created (affects all resources) <code>bool</code> <code>true</code> no create_namespace Create the namespace if it does not yet exist. Defaults to <code>false</code> <code>bool</code> <code>null</code> no create_policy Whether to create an IAM policy that is attached to the IAM role created <code>bool</code> <code>true</code> no create_release Determines whether the Helm release is created <code>bool</code> <code>true</code> no create_role Determines whether to create an IAM role <code>bool</code> <code>false</code> no dependency_update Runs helm dependency update before installing the chart. Defaults to <code>false</code> <code>bool</code> <code>null</code> no description Set release description attribute (visible in the history) <code>string</code> <code>null</code> no devel Use chart development versions, too. Equivalent to version '&gt;0.0.0-0'. If version is set, this is ignored <code>bool</code> <code>null</code> no disable_openapi_validation If set, the installation process will not validate rendered templates against the Kubernetes OpenAPI Schema. Defaults to <code>false</code> <code>bool</code> <code>null</code> no disable_webhooks Prevent hooks from running. Defaults to <code>false</code> <code>bool</code> <code>null</code> no force_update Force resource update through delete/recreate if needed. Defaults to <code>false</code> <code>bool</code> <code>null</code> no keyring Location of public keys used for verification. Used only if verify is true. Defaults to <code>/.gnupg/pubring.gpg</code> in the location set by <code>home</code> <code>string</code> <code>null</code> no lint Run the helm chart linter during the plan. Defaults to <code>false</code> <code>bool</code> <code>null</code> no max_history Maximum number of release versions stored per release. Defaults to <code>3</code>. Normally default is 0 (no limit) <code>number</code> <code>3</code> no max_session_duration Maximum CLI/API session duration in seconds between 3600 and 43200 <code>number</code> <code>null</code> no name Name of the Helm release <code>string</code> <code>\"\"</code> no namespace The namespace to install the release into. Defaults to <code>default</code> <code>string</code> <code>null</code> no oidc_providers Map of OIDC providers where each provider map should contain the <code>provider_arn</code>, and <code>service_accounts</code> <code>any</code> <code>{}</code> no override_policy_documents List of IAM policy documents that are merged together into the exported document. In merging, statements with non-blank <code>sid</code>s will override statements with the same <code>sid</code> <code>list(string)</code> <code>[]</code> no policy_description IAM policy description <code>string</code> <code>null</code> no policy_name Name of IAM policy <code>string</code> <code>null</code> no policy_name_use_prefix Determines whether the IAM policy name (<code>policy_name</code>) is used as a prefix <code>bool</code> <code>false</code> no policy_path Path of IAM policy <code>string</code> <code>null</code> no policy_statements List of IAM policy statements <code>any</code> <code>[]</code> no postrender Configure a command to run after helm renders the manifest which can alter the manifest contents <code>any</code> <code>{}</code> no recreate_pods Perform pods restart during upgrade/rollback. Defaults to <code>false</code> <code>bool</code> <code>null</code> no release_custom_delay_triggers List of resources to trigger the delay of the helm release <code>list(string)</code> <code>[]</code> no release_delay_create_duration The duration to wait before creating the helm release <code>string</code> <code>null</code> no release_delay_destroy_duration The duration to wait before destroying the helm release <code>string</code> <code>null</code> no render_subchart_notes If set, render subchart notes along with the parent. Defaults to <code>true</code> <code>bool</code> <code>null</code> no replace Re-use the given name, only if that name is a deleted release which remains in the history. This is unsafe in production. Defaults to <code>false</code> <code>bool</code> <code>null</code> no repository Repository URL where to locate the requested chart <code>string</code> <code>null</code> no repository_ca_file The Repositories CA File <code>string</code> <code>null</code> no repository_cert_file The repositories cert file <code>string</code> <code>null</code> no repository_key_file The repositories cert key file <code>string</code> <code>null</code> no repository_password Password for HTTP basic authentication against the repository <code>string</code> <code>null</code> no repository_username Username for HTTP basic authentication against the repository <code>string</code> <code>null</code> no reset_values When upgrading, reset the values to the ones built into the chart. Defaults to <code>false</code> <code>bool</code> <code>null</code> no reuse_values When upgrading, reuse the last release's values and merge in any overrides. If <code>reset_values</code> is specified, this is ignored. Defaults to <code>false</code> <code>bool</code> <code>null</code> no role_description IAM Role description <code>string</code> <code>null</code> no role_name Name of IAM role <code>string</code> <code>null</code> no role_name_use_prefix Determines whether the IAM role name (<code>role_name</code>) is used as a prefix <code>bool</code> <code>false</code> no role_path Path of IAM role <code>string</code> <code>\"/\"</code> no role_permissions_boundary_arn Permissions boundary ARN to use for IAM role <code>string</code> <code>null</code> no role_policies Policies to attach to the IAM role in <code>{'static_name' = 'policy_arn'}</code> format <code>map(string)</code> <code>{}</code> no set Value block with custom values to be merged with the values yaml <code>any</code> <code>[]</code> no set_irsa_names Value annotations name where IRSA role ARN created by module will be assigned to the <code>value</code> <code>list(string)</code> <code>[]</code> no set_list Value block with custom values to be merged with the values yaml that are lists <code>any</code> <code>[]</code> no set_sensitive Value block with custom sensitive values to be merged with the values yaml that won't be exposed in the plan's diff <code>any</code> <code>[]</code> no skip_crds If set, no CRDs will be installed. By default, CRDs are installed if not already present. Defaults to <code>false</code> <code>bool</code> <code>null</code> no source_policy_documents List of IAM policy documents that are merged together into the exported document. Statements must have unique <code>sid</code>s <code>list(string)</code> <code>[]</code> no tags A map of tags to add to all resources <code>map(string)</code> <code>{}</code> no timeout Time in seconds to wait for any individual kubernetes operation (like Jobs for hooks). Defaults to <code>300</code> seconds <code>number</code> <code>null</code> no values List of values in raw yaml to pass to helm. Values will be merged, in order, as Helm does with multiple <code>-f</code> options <code>list(string)</code> <code>null</code> no verify Verify the package before installing it. Helm uses a provenance file to verify the integrity of the chart; this must be hosted alongside the chart. For more information see the Helm Documentation. Defaults to <code>false</code> <code>bool</code> <code>null</code> no wait Will wait until all resources are in a ready state before marking the release as successful. If set to <code>true</code>, it will wait for as long as <code>timeout</code>. If set to <code>null</code> fallback on <code>300s</code> timeout.  Defaults to <code>false</code> <code>bool</code> <code>true</code> no wait_for_jobs If wait is enabled, will wait until all Jobs have been completed before marking the release as successful. It will wait for as long as <code>timeout</code>. Defaults to <code>false</code> <code>bool</code> <code>null</code> no"},{"location":"modules/addon/#outputs","title":"Outputs","text":"Name Description app_version The version number of the application being deployed chart The name of the chart create Whether the resources are created iam_policy The policy document iam_policy_arn The ARN assigned by AWS to this policy iam_role_arn ARN of IAM role iam_role_name Name of IAM role iam_role_path Path of IAM role iam_role_unique_id Unique ID of IAM role manifest The manifest of the release name Name is the name of the release namespace Name of Kubernetes namespace revision Version is an int32 which represents the version of the release status The status of the release values The compounded values from <code>values</code> and <code>set*</code> attributes version A SemVer 2 conformant version string of the chart"},{"location":"modules/argocd/","title":"Argocd","text":""},{"location":"modules/argocd/#argocd","title":"ArgoCD","text":"<p>This module deploys ArgoCD as either the hub or spoke controller. This will deploy the default ArgoCD Helm chart and all the necessary IAM roles and policies.</p>"},{"location":"modules/argocd/#examples","title":"Examples","text":""},{"location":"modules/argocd/#hub","title":"Hub","text":"<p>The hub is the controller that manages the spoke clusters. This is where the applications are defined and synced to the spoke clusters.</p> <pre><code>module \"hub\" {\n  source = \"./..\"\n\n  enable_hub = true\n\n  cluster_name = \"example-cluster\"\n}\n</code></pre>"},{"location":"modules/argocd/#spoke","title":"Spoke","text":"<p>The spoke is the controller that manages the applications on the cluster. This is where the applications are deployed and synced from the hub.</p> <pre><code>module \"spoke\" {\n  source = \"./..\"\n\n  enable_spoke = true\n\n  cluster_name = \"example-cluster\"\n\n  cluster_secret_suffix = \"sandbox\"\n\n  hub_iam_role_arn = \"arn:aws:iam::123456789012:role/argocd-example-cluster-hub\"\n}\n</code></pre>"},{"location":"modules/argocd/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.7 aws &gt;= 5.0 helm &gt;= 2.9, &lt; 3.0.0"},{"location":"modules/argocd/#providers","title":"Providers","text":"Name Version aws &gt;= 5.0 helm &gt;= 2.9, &lt; 3.0.0"},{"location":"modules/argocd/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/argocd/#resources","title":"Resources","text":"Name Type aws_eks_access_entry.argocd_spoke resource aws_eks_access_policy_association.argocd_spoke resource aws_eks_pod_identity_association.argocd_application_controller resource aws_eks_pod_identity_association.argocd_applicationset_controller resource aws_eks_pod_identity_association.argocd_server resource aws_iam_policy.argocd_controller resource aws_iam_role.argocd_controller resource aws_iam_role.argocd_spoke resource aws_iam_role_policy_attachment.argocd_controller resource helm_release.argocd resource aws_eks_cluster.cluster data source aws_iam_policy_document.argocd_controller data source aws_iam_policy_document.argocd_controller_assume_role data source aws_iam_policy_document.argocd_spoke data source"},{"location":"modules/argocd/#inputs","title":"Inputs","text":"Name Description Type Default Required cluster_name Name of the EKS cluster <code>string</code> n/a yes create Create the ArgoCD resources <code>bool</code> <code>true</code> no enable_hub Enable ArgoCD Hub <code>bool</code> <code>false</code> no enable_spoke Enable ArgoCD Spoke <code>bool</code> <code>false</code> no helm_set Set values to pass to the Helm chart <pre>list(object({    name  = string    value = string  }))</pre> <code>[]</code> no helm_values Values to pass to the Helm chart <code>list(string)</code> <code>[]</code> no helm_version Version of the Helm chart to install <code>string</code> <code>\"7.8.26\"</code> no hub_iam_role_arn (Deprecated, use hub_iam_role_arns) IAM Role ARN for ArgoCD Hub. This is required for spoke clusters <code>string</code> <code>null</code> no hub_iam_role_arns A list of ArgoCD Hub IAM Role ARNs, enabling hubs to access spoke clusters. This is required for spoke clusters. <code>list(string)</code> <code>null</code> no hub_iam_role_name IAM Role Name for ArgoCD Hub. This is referenced by the Spoke clusters <code>string</code> <code>\"argocd-controller\"</code> no namespace Namespace to deploy ArgoCD <code>string</code> <code>\"argocd\"</code> no tags A map of tags to add to all resources <code>map(string)</code> <code>{}</code> no"},{"location":"modules/argocd/#outputs","title":"Outputs","text":"Name Description cluster_name Name of the EKS cluster hub_iam_role_arn IAM Role ARN for ArgoCD spoke_iam_role_arn IAM Role ARN for ArgoCD Spoke"},{"location":"modules/cloudflare/","title":"Cloudflare","text":""},{"location":"modules/cloudflare/#cloudflare-zone-delegation","title":"Cloudflare zone delegation","text":"<p>Deploy the Cloudflare delegation</p> <pre><code>module \"cloudflare\" {\n  source   = \"tx-pts-dai/kubernetes-platform/aws//modules/cloudflare\"\n  version  = ...\n  for_each = var.zones\n\n  account_id   = jsondecode(data.aws_secretsmanager_secret_version.cloudflare.secret_string)[\"accountId\"]\n  name_servers = module.route53_zones[each.key].route53_zone_name_servers[each.key]\n  domain_name  = module.route53_zones[each.key].route53_zone_name[each.key]\n}\n</code></pre>"},{"location":"modules/cloudflare/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 cloudflare &gt;= 4.0, &lt; 5.0"},{"location":"modules/cloudflare/#providers","title":"Providers","text":"Name Version cloudflare &gt;= 4.0, &lt; 5.0"},{"location":"modules/cloudflare/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/cloudflare/#resources","title":"Resources","text":"Name Type cloudflare_record.ns resource cloudflare_zone.this data source"},{"location":"modules/cloudflare/#inputs","title":"Inputs","text":"Name Description Type Default Required account_id Cloudflare account id <code>string</code> n/a yes comment Record comment <code>string</code> <code>\"Managed by Terraform\"</code> no name_servers List of name servers to delegate to Cloudflare <code>list(string)</code> n/a yes zone_name The domain name to delegate in Cloudflare <code>string</code> n/a yes"},{"location":"modules/cloudflare/#outputs","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/datadog/","title":"Datadog","text":""},{"location":"modules/datadog/#datadog-operator","title":"Datadog Operator","text":"<p>Deploy the Datadog Operator and the Datadog Agent</p> <pre><code>module \"datadog\" {\n  source = \"../../modules/datadog\"\n\n  cluster_name   = \"my-cluster\"\n  datadog_secret = \"secretsmanager/secret/namespace\"\n  environment    = \"example\"\n  product_name   = \"dai\"\n\n  datadog_operator_helm_values = {\n    values = [\n      &lt;&lt;-YAML\n      remoteConfiguration:\n        enabled: true\n      YAML\n    ]\n  }\n\n  datadog_operator_helm_set = [\n    {\n      name  = \"replicas\"\n      value = 2\n    }\n  ]\n\n  datadog_agent_helm_values = [\n    &lt;&lt;-YAML\n    spec:\n      override:\n        clusterAgent:\n          replicas: 1\n    YAML\n  ]\n\n  datadog_agent_helm_set = [\n    {\n      name  = \"spec.features.admissionController.agentSidecarInjection.image.tag\",\n      value = \"7.57.2\"\n    }\n  ]\n}\n</code></pre>"},{"location":"modules/datadog/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.0 helm &gt;= 2.6, &lt; 3.0.0 kubectl &gt;= 2.0.2 kubernetes &gt;= 2.27 time &gt;= 0.11.0"},{"location":"modules/datadog/#providers","title":"Providers","text":"Name Version helm &gt;= 2.6, &lt; 3.0.0 kubectl &gt;= 2.0.2 kubernetes &gt;= 2.27 time &gt;= 0.11.0"},{"location":"modules/datadog/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/datadog/#resources","title":"Resources","text":"Name Type helm_release.datadog_agent resource helm_release.datadog_operator resource helm_release.datadog_secrets resource helm_release.datadog_secrets_fargate resource kubectl_manifest.fargate_cluster_role resource kubectl_manifest.fargate_role_binding resource kubernetes_annotations.this resource time_sleep.this resource"},{"location":"modules/datadog/#inputs","title":"Inputs","text":"Name Description Type Default Required cluster_name Name of the cluster <code>string</code> n/a yes datadog_agent_helm_set List of Datadog Agent custom resource set values <pre>list(object({    name  = string    value = string  }))</pre> <code>[]</code> no datadog_agent_helm_values List of Datadog Agent custom resource values <code>list(string)</code> <code>[]</code> no datadog_operator_helm_set List of Datadog Operator Helm set values <pre>list(object({    name  = string    value = string  }))</pre> <code>[]</code> no datadog_operator_helm_values List of Datadog Operator Helm values <code>list(string)</code> <code>[]</code> no datadog_operator_helm_version Version of the datadog operator chart <code>string</code> <code>\"2.9.2\"</code> no datadog_secret Name of the datadog secret in Secrets Manager <code>string</code> n/a yes environment Name of the environment <code>string</code> n/a yes namespace Namespace for Datadog resources <code>string</code> <code>\"monitoring\"</code> no product_name Value of the product tag added to all metrics and logs sent to datadog <code>string</code> n/a yes"},{"location":"modules/datadog/#outputs","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/datadog/#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.0.0 datadog ~&gt; 3.39 helm ~&gt; 2.6 kubernetes &gt;= 2.0.0"},{"location":"modules/datadog/#providers_1","title":"Providers","text":"Name Version datadog ~&gt; 3.39 helm ~&gt; 2.6 kubernetes &gt;= 2.0.0"},{"location":"modules/datadog/#modules_1","title":"Modules","text":"Name Source Version datadog_operator aws-ia/eks-blueprints-addon/aws ~&gt; 1.0"},{"location":"modules/datadog/#resources_1","title":"Resources","text":"Name Type datadog_api_key.datadog_agent resource datadog_application_key.datadog_agent resource helm_release.datadog_agent resource kubernetes_secret.datadog_keys resource"},{"location":"modules/datadog/#inputs_1","title":"Inputs","text":"Name Description Type Default Required cluster_name Name of the cluster <code>string</code> n/a yes datadog Object of Datadog configurations <pre>object({    agent_api_key_name            = optional(string) # by default it uses the cluster name    agent_app_key_name            = optional(string) # by default it uses the cluster name    operator_chart_version        = optional(string)    custom_resource_chart_version = optional(string)  })</pre> <code>{}</code> no datadog_agent_helm_values List of Datadog Agent custom resource values. https://github.com/DataDog/datadog-operator/blob/main/docs/configuration.v2alpha1.md <pre>list(object({    name  = string    value = string  }))</pre> <code>[]</code> no datadog_operator_helm_values List of Datadog Operator values <pre>list(object({    name  = string    value = string  }))</pre> <pre>[  {    \"name\": \"resources.requests.cpu\",    \"value\": \"10m\"  },  {    \"name\": \"resources.requests.memory\",    \"value\": \"50Mi\"  }]</pre> no namespace Namespace for Datadog resources <code>string</code> <code>\"monitoring\"</code> no"},{"location":"modules/datadog/#outputs_1","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/datadog/#requirements_2","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.0.0 datadog ~&gt; 3.39 helm ~&gt; 2.6 kubernetes &gt;= 2.0.0"},{"location":"modules/datadog/#providers_2","title":"Providers","text":"Name Version datadog ~&gt; 3.39 helm ~&gt; 2.6 kubernetes &gt;= 2.0.0"},{"location":"modules/datadog/#modules_2","title":"Modules","text":"Name Source Version datadog_operator aws-ia/eks-blueprints-addon/aws ~&gt; 1.0"},{"location":"modules/datadog/#resources_2","title":"Resources","text":"Name Type datadog_api_key.datadog_agent resource datadog_application_key.datadog_agent resource helm_release.datadog_agent resource kubernetes_secret.datadog_keys resource"},{"location":"modules/datadog/#inputs_2","title":"Inputs","text":"Name Description Type Default Required cluster_name Name of the cluster <code>string</code> n/a yes datadog Object of Datadog configurations <pre>object({    agent_api_key_name            = optional(string) # by default it uses the cluster name    agent_app_key_name            = optional(string) # by default it uses the cluster name    operator_chart_version        = optional(string)    custom_resource_chart_version = optional(string)  })</pre> <code>{}</code> no datadog_agent_helm_values List of Datadog Agent custom resource values. https://github.com/DataDog/datadog-operator/blob/main/docs/configuration.v2alpha1.md <pre>list(object({    name  = string    value = string  }))</pre> <code>[]</code> no datadog_operator_helm_values List of Datadog Operator values <pre>list(object({    name  = string    value = string  }))</pre> <pre>[  {    \"name\": \"resources.requests.cpu\",    \"value\": \"10m\"  },  {    \"name\": \"resources.requests.memory\",    \"value\": \"50Mi\"  }]</pre> no namespace Namespace for Datadog resources <code>string</code> <code>\"monitoring\"</code> no"},{"location":"modules/datadog/#outputs_2","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/lacework/","title":"Lacework","text":""},{"location":"modules/lacework/#lacework","title":"Lacework","text":"<p>Deploy Lacework Agents</p> <pre><code>module \"lacework\" {\n  source  = \"tx-pts-dai/kubernetes-platform/aws//modules/datadog\"\n  version = ...\n\n  cluster_name = module.eks.cluster_name\n}\n</code></pre>"},{"location":"modules/lacework/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.0 kubernetes &gt;= 2.0.0 lacework &gt;= 2.0.0"},{"location":"modules/lacework/#providers","title":"Providers","text":"Name Version aws &gt;= 5.0 kubernetes &gt;= 2.0.0 lacework &gt;= 2.0.0"},{"location":"modules/lacework/#modules","title":"Modules","text":"Name Source Version lacework_k8s_datacollector lacework/agent/kubernetes 2.5.2"},{"location":"modules/lacework/#resources","title":"Resources","text":"Name Type kubernetes_namespace_v1.lacework resource lacework_agent_access_token.kubernetes resource aws_caller_identity.this data source"},{"location":"modules/lacework/#inputs","title":"Inputs","text":"Name Description Type Default Required agent_tags A map/dictionary of Tags to be assigned to the Lacework datacollector <code>map(string)</code> <code>{}</code> no cluster_name Name of the cluster <code>string</code> n/a yes enable_cluster_agent A boolean representing whether the Lacework cluster agent should be deployed <code>bool</code> <code>true</code> no namespace Namespace for Lacework resources <code>string</code> <code>\"lacework\"</code> no node_affinity Node affinity settings <pre>list(object({    key      = string    operator = string    values   = list(string)  }))</pre> <pre>[  {    \"key\": \"eks.amazonaws.com/compute-type\",    \"operator\": \"NotIn\",    \"values\": [      \"fargate\"    ]  }]</pre> no pod_priority_class_name Name of the pod priority class <code>string</code> <code>\"system-node-critical\"</code> no resources Resources for the Lacework agent <pre>object({    cpu_request = string    mem_request = string    cpu_limit   = string    mem_limit   = string  })</pre> <pre>{  \"cpu_limit\": \"1000m\",  \"cpu_request\": \"100m\",  \"mem_limit\": \"1024Mi\",  \"mem_request\": \"256Mi\"}</pre> no server_url Lacework server URL <code>string</code> <code>\"https://api.fra.lacework.net\"</code> no tolerations Tolerations for the Lacework agent <code>list(map(string))</code> <pre>[  {    \"effect\": \"NoSchedule\",    \"operator\": \"Exists\"  }]</pre> no"},{"location":"modules/lacework/#outputs","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/lacework/#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.0.0 kubernetes &gt;= 2.0.0 lacework &gt;= 1.18.2"},{"location":"modules/lacework/#providers_1","title":"Providers","text":"Name Version aws &gt;= 5.0.0 kubernetes &gt;= 2.0.0 lacework &gt;= 1.18.2"},{"location":"modules/lacework/#modules_1","title":"Modules","text":"Name Source Version lacework_k8s_datacollector lacework/agent/kubernetes 2.5.1"},{"location":"modules/lacework/#resources_1","title":"Resources","text":"Name Type kubernetes_namespace_v1.lacework resource lacework_agent_access_token.kubernetes resource aws_caller_identity.this data source"},{"location":"modules/lacework/#inputs_1","title":"Inputs","text":"Name Description Type Default Required agent_tags A map/dictionary of Tags to be assigned to the Lacework datacollector <code>map(string)</code> <code>{}</code> no cluster_name Name of the cluster <code>string</code> n/a yes enable_cluster_agent A boolean representing whether the Lacework cluster agent should be deployed <code>bool</code> <code>true</code> no namespace Namespace for Lacework resources <code>string</code> <code>\"lacework\"</code> no node_affinity Node affinity settings <pre>list(object({    key      = string    operator = string    values   = list(string)  }))</pre> <pre>[  {    \"key\": \"eks.amazonaws.com/compute-type\",    \"operator\": \"NotIn\",    \"values\": [      \"fargate\"    ]  }]</pre> no pod_priority_class_name Name of the pod priority class <code>string</code> <code>\"system-node-critical\"</code> no resources Resources for the Lacework agent <pre>object({    cpu_request = string    mem_request = string    cpu_limit   = string    mem_limit   = string  })</pre> <pre>{  \"cpu_limit\": \"1000m\",  \"cpu_request\": \"100m\",  \"mem_limit\": \"1024Mi\",  \"mem_request\": \"256Mi\"}</pre> no server_url Lacework server URL <code>string</code> <code>\"https://api.fra.lacework.net\"</code> no tolerations Tolerations for the Lacework agent <code>list(map(string))</code> <pre>[  {    \"effect\": \"NoSchedule\",    \"operator\": \"Exists\"  }]</pre> no"},{"location":"modules/lacework/#outputs_1","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/network/","title":"Network","text":""},{"location":"modules/network/#network-module","title":"Network Module","text":"<p>This module is a wrapper around the public VPC module with some additional configuration options suitable for the Tamedia platform.</p> <p>See the network example how to use it and how to retrieve informations on the created resources from another stack.</p>"},{"location":"modules/network/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.42"},{"location":"modules/network/#providers","title":"Providers","text":"Name Version aws &gt;= 5.42"},{"location":"modules/network/#modules","title":"Modules","text":"Name Source Version ssm ./../ssm n/a vpc terraform-aws-modules/vpc/aws 5.21.0"},{"location":"modules/network/#resources","title":"Resources","text":"Name Type aws_availability_zones.available data source"},{"location":"modules/network/#inputs","title":"Inputs","text":"Name Description Type Default Required az_count Number of availability zones to use <code>number</code> <code>3</code> no cidr The CIDR block for the VPC <code>string</code> <code>\"10.0.0.0/16\"</code> no create_vpc Create the VPC <code>bool</code> <code>true</code> no enable_nat_gateway Enable NAT Gateways <code>bool</code> <code>true</code> no single_nat_gateway Use a single NAT Gateway <code>bool</code> <code>true</code> no stack_name The stack name for the resources <code>string</code> n/a yes subnet_configs List of networks objects with their name and size in bits. The order of the list should not change. <code>list(map(number))</code> <pre>[  {    \"public\": 24  },  {    \"private\": 24  },  {    \"intra\": 26  },  {    \"database\": 26  },  {    \"redshift\": 26  },  {    \"karpenter\": 22  }]</pre> no tags A map of tags to add to all resources <code>map(string)</code> <code>{}</code> no"},{"location":"modules/network/#outputs","title":"Outputs","text":"Name Description cidr The base CIDR block for the VPC grouped_networks A map of subnet names to their respective details and list of CIDR blocks. network_cidr_blocks A map from network names to allocated address prefixes in CIDR notation. networks A list of network objects with name, az, hosts, and cidr_block. vpc Map of attributes for the VPC"},{"location":"modules/network/#requirements_1","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.42.0"},{"location":"modules/network/#providers_1","title":"Providers","text":"Name Version aws &gt;= 5.42.0"},{"location":"modules/network/#modules_1","title":"Modules","text":"Name Source Version vpc terraform-aws-modules/vpc/aws 5.8.1"},{"location":"modules/network/#resources_1","title":"Resources","text":"Name Type aws_availability_zones.available data source"},{"location":"modules/network/#inputs_1","title":"Inputs","text":"Name Description Type Default Required az_count Number of availability zones to use <code>number</code> <code>3</code> no cidr The CIDR block for the VPC <code>string</code> <code>\"10.0.0.0/16\"</code> no create_vpc Create the VPC <code>bool</code> <code>true</code> no enable_nat_gateway Enable NAT Gateways <code>bool</code> <code>true</code> no single_nat_gateway Use a single NAT Gateway <code>bool</code> <code>true</code> no stack_name The stack name for the resources <code>string</code> n/a yes subnet_configs List of networks objects with their name and size in bits. The order of the list should not change. <code>list(map(number))</code> <pre>[  {    \"public\": 24  },  {    \"private\": 24  },  {    \"intra\": 26  },  {    \"database\": 26  },  {    \"redshift\": 26  },  {    \"karpenter\": 22  }]</pre> no tags A map of tags to add to all resources <code>map(string)</code> <code>{}</code> no"},{"location":"modules/network/#outputs_1","title":"Outputs","text":"Name Description cidr The base CIDR block for the VPC grouped_networks A map of subnet names to their respective details and list of CIDR blocks. network_cidr_blocks A map from network names to allocated address prefixes in CIDR notation. networks A list of network objects with name, az, hosts, and cidr_block. vpc Map of attributes for the VPC"},{"location":"modules/security-group/","title":"Security group","text":""},{"location":"modules/security-group/#security-group-module","title":"Security Group Module","text":"<p>This module creates a security group and rules.</p>"},{"location":"modules/security-group/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.7.0 aws &gt;= 5.0"},{"location":"modules/security-group/#providers","title":"Providers","text":"Name Version aws &gt;= 5.0"},{"location":"modules/security-group/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/security-group/#resources","title":"Resources","text":"Name Type aws_security_group.this resource aws_security_group_rule.this resource"},{"location":"modules/security-group/#inputs","title":"Inputs","text":"Name Description Type Default Required create Create the security group. <code>bool</code> <code>true</code> no description The description of the security group. <code>string</code> <code>\"\"</code> no egress_rules The egress rules for the security group. <pre>map(object({    type                     = string    protocol                 = string    from_port                = number    to_port                  = number    description              = optional(string)    cidr_blocks              = optional(list(string))    ipv6_cidr_blocks         = optional(list(string))    prefix_list_ids          = optional(list(string))    self                     = optional(bool)    source_security_group_id = optional(string)  }))</pre> <code>{}</code> no ingress_rules The ingress rules for the security group. <pre>map(object({    type                     = string    protocol                 = string    from_port                = number    to_port                  = number    description              = optional(string)    cidr_blocks              = optional(list(string))    ipv6_cidr_blocks         = optional(list(string))    prefix_list_ids          = optional(list(string))    self                     = optional(bool)    source_security_group_id = optional(string)  }))</pre> <code>{}</code> no name The name of the security group, this name must be unique within the VPC. <code>string</code> n/a yes tags A map of tags to add to all resources. <code>map(string)</code> <code>{}</code> no vpc_id The VPC id to create the security group in. <code>string</code> n/a yes"},{"location":"modules/security-group/#outputs","title":"Outputs","text":"<p>No outputs.</p>"},{"location":"modules/ssm/","title":"Ssm","text":""},{"location":"modules/ssm/#ssm-parameter-store-for-terraform-outputs","title":"SSM Parameter Store for Terraform Outputs","text":""},{"location":"modules/ssm/#overview","title":"Overview","text":"<p>This Terraform module is designed to store Terraform outputs in AWS Systems Manager (SSM) Parameter Store and provide functionality to look up parameters for cross stack reference. It helps store and retrieve parameters across different terraform stacks and environments.</p>"},{"location":"modules/ssm/#features","title":"Features","text":"<ul> <li>Store parameters in SSM Parameter Store with customizable hierarchies.</li> <li>Retrieve parameters from SSM Parameter Store based on specified paths.</li> <li>Retrieve list of stack names and parameters.</li> <li>Filter stacks with prefixes.</li> <li>Support for securely storing sensitive parameters.</li> <li>Dynamically identify and retrieve the latest stack parameters.</li> </ul>"},{"location":"modules/ssm/#example","title":"Example","text":"<p>Store parameters in SSM Parameter Store: <pre><code>module \"ssm_parameters\" {\n  source           = \"./path-to-your-module\"\n\n  base_prefix      = \"infrastructure\"\n  stack_type       = \"platform\"\n  stack_name       = \"stack-123\"\n\n  parameters = {\n    cluster_endpoint = {\n      type           = \"String\"\n      insecure_value = \"https://cluster-zxcv.local\"\n    }\n    cluster_name = {\n      insecure_value = \"cluster-123\"\n    }\n  }\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n</code></pre></p> <p>Retrieve parameters from SSM Parameter Store: <pre><code>module \"ssm_lookup\" {\n  source           = \"./path-to-your-module\"\n\n  base_prefix       = \"infrastructure\"\n  stack_type        = \"platform\"\n  stack_name_prefix = \"stack-\"\n\n  lookup = [\n    \"cluster_endpoint\",\n    \"cluster_name\"\n  ]\n\n  tags = {\n    Environment = \"dev\"\n    Terraform   = \"true\"\n  }\n}\n</code></pre></p> <p>Outputs: <pre><code>ssm_lookup = {\n  \"filtered_parameters\" = {\n    \"/infrastructure/platform/stack-123/cluster_endpoint\" = \"cluster-zxcv\"\n    \"/infrastructure/platform/stack-123/cluster_name\" = \"foo\"\n    \"/infrastructure/platform/stack-234/cluster_endpoint\" = \"cluster-asjf\"\n    \"/infrastructure/platform/stack-234/cluster_name\" = \"bar\"\n  }\n  \"latest_stack_parameters\" = {\n    \"/infrastructure/platform/stack-234/cluster_endpoint\" = \"https://cluster-asjf.local\"\n    \"/infrastructure/platform/stack-234/cluster_name\" = \"bar\"\n  }\n  \"lookup\" = {\n    \"stack-123\" = {\n      \"cluster_endpoint\" = \"https://cluster-zxcv.local\"\n      \"cluster_name\" = \"foo\"\n    }\n    \"stack-234\" = {\n      \"cluster_endpoint\" = \"https://cluster-asjf.local\"\n      \"cluster_name\" = \"bar\"\n    }\n  }\n  # All parameters stored in SSM\n  \"parameters\" = tomap({\n    \"/infrastructure/platform/stack-123/cluster_endpoint\" = \"https://cluster-zxcv.local\"\n    \"/infrastructure/platform/stack-123/cluster_name\" = \"foo\"\n    \"/infrastructure/platform/stack-234/cluster_endpoint\" = \"https://cluster-asjf.local\"\n    \"/infrastructure/platform/stack-234/cluster_name\" = \"bar\"\n  })\n  \"stacks\" = tolist([\n    \"stack-123\",\n    \"stack-234\",\n  ])\n}\n</code></pre></p>"},{"location":"modules/ssm/#implementation-details","title":"Implementation Details","text":""},{"location":"modules/ssm/#storing-parameters","title":"Storing Parameters","text":"<p>Parameters are stored in SSM Parameter Store using the <code>aws_ssm_parameter</code> resource. The parameter name is constructed using the <code>base_prefix</code>, <code>stack_type</code>, and <code>stack_name</code>, forming a hierarchy.</p>"},{"location":"modules/ssm/#retrieving-parameters","title":"Retrieving Parameters","text":"<p>The module uses the <code>aws_ssm_parameters_by_path</code> data source to retrieve parameters from SSM Parameter Store based on the specified path. The retrieved parameters are processed to:</p> <ul> <li>Filter parameters by stack name prefix.</li> <li>Extract unique stack names.</li> <li>Create a lookup map for stack-specific parameters.</li> <li>Identify and retrieve the latest stack parameters.</li> </ul>"},{"location":"modules/ssm/#filtering-and-lookup","title":"Filtering and Lookup","text":"<p>The <code>filtered_parameters</code> local variable is used to filter parameters based on the stack name prefix. The <code>lookup</code> local variable creates a nested map of stack-specific parameters based on the provided lookup list. The <code>latest_stack_parameters</code> local variable identifies and retrieves parameters for the last created stack since we use timestamps in the stack names suffix.</p>"},{"location":"modules/ssm/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 1.3.2 aws &gt;= 5.42"},{"location":"modules/ssm/#providers","title":"Providers","text":"Name Version aws &gt;= 5.42"},{"location":"modules/ssm/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"modules/ssm/#resources","title":"Resources","text":"Name Type aws_ssm_parameter.cluster_name resource aws_ssm_parameters_by_path.this data source"},{"location":"modules/ssm/#inputs","title":"Inputs","text":"Name Description Type Default Required base_prefix Base SSM namespace prefix for the parameters <code>string</code> <code>\"infrastructure\"</code> no create Create the SSM parameters <code>bool</code> <code>true</code> no lookup List of parameters to Lookup <code>list(any)</code> <code>[]</code> no parameters Map of SSM parameters to create <pre>map(object({    name           = optional(string)    type           = optional(string, \"String\")    value          = optional(string)    insecure_value = optional(string)  }))</pre> <code>{}</code> no stack_name The name of the stack <code>string</code> <code>null</code> no stack_name_prefix Filter all stacks that include this prefix in the name. <code>string</code> <code>\"\"</code> no stack_type The type of terraform stack to be used in the namespace prefix. platform, network, account, shared <code>string</code> <code>\"\"</code> no tags Default tags to apply to all resources <code>map(string)</code> <code>{}</code> no"},{"location":"modules/ssm/#outputs","title":"Outputs","text":"Name Description filtered_parameters List of parameters filtered by stack name prefix latest_stack_parameters Latest created stack parameters lookup Map of parameters from filtered parameters containing only keys defined in lookup parameters All parameters defined in SSM stacks List of stacks defined in SSM ordered by creation date (latest first)"},{"location":"modules/ssm/#contributions","title":"Contributions","text":"<p>Contributions to enhance the functionality and flexibility of this module are welcome. Please submit a pull request or open an issue to discuss any changes.</p>"},{"location":"modules/ssm/#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"}]}